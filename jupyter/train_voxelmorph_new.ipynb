{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "../cnn_utils/vis_utils.py:14: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  mpl.use('Agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 160, 192, 224, 1), (None, 160, 192, 224, 3)]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from dataset_utils import adni_loader\n",
    "#from networks import transform_network_utils\n",
    "\n",
    "sys.path.append('../neuron')\n",
    "sys.path.append('../voxelmorph')\n",
    "import src.networks as vm_networks\n",
    "\n",
    "sys.path.append('../voxelmorph-sandbox/')\n",
    "from voxelmorph.dense_3D_spatial_transformer import Dense3DSpatialTransformer\n",
    "\n",
    "\n",
    "gpu_ids = [3]\n",
    "# set gpu id and tf settings\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(g) for g in gpu_ids])\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "start_iter = 0\n",
    "nf_enc = [16, 32, 32, 32]\n",
    "nf_dec = [32, 32, 32, 32, 32, 16, 16]\n",
    "# vm2 model\n",
    "voxelmorph_model = vm_networks.cvpr2018_net(\n",
    "    vol_size=(160, 192, 224),\n",
    "    enc_nf=nf_enc, \n",
    "    dec_nf=nf_dec,\n",
    "    indexing='xy',\n",
    ")\n",
    "\n",
    "# # reset weights to random\n",
    "# from keras.initializers import glorot_uniform\n",
    "# session = K.get_session()\n",
    "# initial_weights = voxelmorph_model.get_weights()\n",
    "# new_weights = [glorot_uniform()(w.shape).eval(session=session) for w in initial_weights]\n",
    "# voxelmorph_model.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cnn_utils/vis_utils.py:14: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n",
      "  mpl.use('Agg')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading adni dataset adni-unnorm-masked_10ul_subj-OASIS_OAS1_0327-l\n",
      "Params: {'dataset_name': 'adni', 'source_name': 'atl', 'target_name': 'subjs', 'unnormalized': True, 'masked': True, 'n_shot': 1, 'use_atlas_as_source': False, 'use_subject': 'OASIS_OAS1_0327_MR1_mri_talairach_orig', 'img_shape': (160, 192, 224, 1), 'pred_img_shape': (160, 192, 1), 'aug_img_shape': (160, 192, 224, 1), 'n_unlabeled': 10, 'n_validation': 50, 'load_vols': True, 'aug_in_gen': True, 'n_vte_aug': None, 'n_flow_aug': None, 'use_labels': [0, 16, 10, 49, 8, 47, 4, 43, 7, 46, 12, 51, 2, 41, 28, 60, 11, 50, 13, 52, 17, 53, 14, 15, 18, 54, 24, 3, 42, 31, 63], 'final_test': False, 'warp_labels': True, 'n_dims': 3, 'orig_img_shape': (160, 192, 224, 1), 'scale': 1.0, 'split_id': None}\n",
      "Got list of 7329 files from /data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/*.npz:\n",
      "ADNI_ADNI-3T-FS-5.3-Long_293689.long.016_S_4591_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-3T-FS-5.3-Long_78841.long.016_S_1326_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-1.5T-FS-5.3-Long_436815.long.094_S_1330_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-3T-FS-5.3-Long_296323.long.068_S_2168_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-3T-FS-5.3-Long_388923.long.135_S_5273_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-3T-FS-5.3-Long_272700.long.009_S_4388_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-1.5T-FS-5.3-Long_394785.long.027_S_0408_base_mri_talairach_orig.npz\n",
      "PPMI_3519_mri_talairach_orig.npz\n",
      "ADNI_ADNI-3T-FS-5.3-Long_119158.long.053_S_0507_base_mri_talairach_orig.npz\n",
      "ADNI_ADNI-1.5T-FS-5.3-Long_63306.long.007_S_0249_base_mri_talairach_orig.npz\n",
      "ABIDE_50685_mri_talairach_orig.npz\n",
      "ADNI_ADNI-1.5T-FS-5.3-Long_121666.long.041_S_1423_base_mri_talairach_orig.npz\n",
      "GSP_120719_TT88SP_FS_mri_talairach_orig.npz\n",
      "COBRE_0040043_mri_talairach_orig.npz\n",
      "ADNI_ADNI-3T-FS-5.3-Long_416015.long.021_S_2124_base_mri_talairach_orig.npz\n",
      "...\n",
      "Got 12 training and 50 validation files!\n",
      "Loaded 0 of 1 files\n",
      "Labeled train vols:\n",
      "X_labeled_train: (1, 160, 192, 224, 1)\n",
      "Y_labeled_train: (1, 160, 192, 224)\n",
      "ids_labeled_train: ['OASIS_OAS1_0327_MR1_mri_talairach_orig']\n",
      "Loaded 0 of 50 files\n",
      "Loaded 0 of 11 files\n",
      "Filtering labels to [0, 16, 10, 49, 8, 47, 4, 43, 7, 46, 12, 51, 2, 41, 28, 60, 11, 50, 13, 52, 17, 53, 14, 15, 18, 54, 24, 3, 42, 31, 63]\n"
     ]
    }
   ],
   "source": [
    "dataset_key = 'adni-10-csts2'\n",
    "import vte_runner\n",
    "data_params = vte_runner.named_vte_data_params[dataset_key]\n",
    "    \n",
    "ds = adni_loader.ADNIDataset(data_params)\n",
    "\n",
    "vol_shape = tuple(data_params['img_shape'])\n",
    "\n",
    "# just load some examples so we can get the image size, but actually use a generator later...\n",
    "(X_unlabeled, _, ids_unlabeled),\\\n",
    "(X_labeled_train, Y_labeled_train, ids_labeled_train), \\\n",
    "(X_labeled_valid, Y_labeled_valid, ids_labeled_valid), \\\n",
    "label_mapping \\\n",
    "= ds.load_dataset(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 160, 192, 224, 1)\n",
      "(1, 160, 192, 224, 1)\n",
      "(50, 160, 192, 224, 1)\n",
      "['/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/OASIS_OAS1_0327_MR1_mri_talairach_orig.npz']\n",
      "True\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=192x160 at 0x7FEF8472FBE0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=192x160 at 0x7FEF8472FCF8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_unlabeled.shape)\n",
    "print(X_labeled_train.shape)\n",
    "print(X_labeled_valid.shape)\n",
    "print(ds.files_labeled_train)\n",
    "print(ds.params['load_vols'])\n",
    "import IPython\n",
    "import PIL\n",
    "IPython.display.display(PIL.Image.fromarray((X_labeled_train[0, :, :, 64, 0]*255).astype(np.uint8)))\n",
    "IPython.display.display(PIL.Image.fromarray((X_unlabeled[0, :, :, 64, 0]*255).astype(np.uint8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 160, 192, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 160, 192, 224 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 160, 192, 224 0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 80, 96, 112,  880         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 80, 96, 112,  0           conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 40, 48, 56, 3 13856       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 40, 48, 56, 3 0           conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 20, 24, 28, 3 27680       leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 20, 24, 28, 3 0           conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 10, 12, 14, 3 27680       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 10, 12, 14, 3 0           conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 10, 12, 14, 3 27680       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 10, 12, 14, 3 0           conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 20, 24, 28, 3 0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20, 24, 28, 6 0           up_sampling3d_1[0][0]            \n",
      "                                                                 leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 20, 24, 28, 3 55328       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 20, 24, 28, 3 0           conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3D)  (None, 40, 48, 56, 3 0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 40, 48, 56, 6 0           up_sampling3d_2[0][0]            \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 40, 48, 56, 3 55328       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 40, 48, 56, 3 0           conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_3 (UpSampling3D)  (None, 80, 96, 112,  0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 80, 96, 112,  0           up_sampling3d_3[0][0]            \n",
      "                                                                 leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_8 (Conv3D)               (None, 80, 96, 112,  41504       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 80, 96, 112,  0           conv3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_9 (Conv3D)               (None, 80, 96, 112,  27680       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 80, 96, 112,  0           conv3d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_4 (UpSampling3D)  (None, 160, 192, 224 0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 160, 192, 224 0           up_sampling3d_4[0][0]            \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_10 (Conv3D)              (None, 160, 192, 224 14704       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 160, 192, 224 0           conv3d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_11 (Conv3D)              (None, 160, 192, 224 6928        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 160, 192, 224 0           conv3d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flow (Conv3D)                   (None, 160, 192, 224 1299        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "spatial_transformer_1 (SpatialT [(None, 160, 192, 22 0           input_1[0][0]                    \n",
      "                                                                 flow[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 300,547\n",
      "Trainable params: 300,547\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=192x160 at 0x7FEF8C0DAEF0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "voxelmorph_model.summary()\n",
    "import data_utils\n",
    "sys.path.append('../voxelmorph')\n",
    "import src.losses as vm_losses\n",
    "# just train voxelmorph\n",
    "voxelmorph_model.compile(\n",
    "    #loss=['mean_squared_error', vm_losses.gradientLoss('l2')],\n",
    "    loss=[vm_losses.NCC().loss, vm_losses.Grad('l2').loss],\n",
    "    #loss_weights=[1.0, ,0.01],\n",
    "    loss_weights=[1.0, 1.],#0.01],\n",
    "    optimizer=Adam(0.0001)\n",
    ")\n",
    "\n",
    "#source_X = ds.X_atlas\n",
    "source_X, _ = adni_loader._load_vol_and_seg(ds.files_labeled_train[0], load_seg=False, mask_vol=ds.params['masked'])\n",
    "source_X = source_X[np.newaxis]\n",
    "IPython.display.display(PIL.Image.fromarray((source_X[0, :, :, 64, 0]*255).astype(np.uint8)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/OASIS_OAS1_0327_MR1_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-3T-FS-5.3-Long_263697.long.153_S_4077_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-3T-FS-5.3-Long_223532.long.153_S_2109_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-1.5T-FS-5.3-Long_76615.long.021_S_0984_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-3T-FS-5.3-Long_451346.long.009_S_0751_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/GSP_100329_NW33DK_FS_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-3T-FS-5.3-Long_282668.long.002_S_4270_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/GSP_110314_JD99RH_FS_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ABIDE_50558_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-3T-FS-5.3-Long_424046.long.073_S_2225_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-1.5T-FS-5.3-Long_123092.long.036_S_0672_base_mri_talairach_orig.npz', '/data/ddmg/voxelmorph/data/t1_mix/proc/resize256-crop_x32/train/origs/ADNI_ADNI-1.5T-FS-5.3-Long_65343.long.027_S_0850_base_mri_talairach_orig.npz']\n",
      "Iter 0, loss [-0.050202467, -0.050202467, 1.7626876e-11]\n",
      "Iter 1, loss [-0.0405689, -0.04056891, 1.1661321e-08]\n",
      "Iter 2, loss [-0.05740692, -0.057406977, 5.4944188e-08]\n",
      "Iter 3, loss [-0.05293878, -0.05293896, 1.7886859e-07]\n",
      "Iter 4, loss [-0.05081292, -0.050813254, 3.348896e-07]\n",
      "Iter 5, loss [-0.32226524, -0.32226616, 9.3378975e-07]\n",
      "Iter 6, loss [-0.05841053, -0.058411468, 9.405309e-07]\n",
      "Iter 7, loss [-0.053988613, -0.053990092, 1.4795769e-06]\n",
      "Iter 8, loss [-0.042407952, -0.042410713, 2.7590527e-06]\n",
      "Iter 9, loss [-0.052336905, -0.052339576, 2.6722944e-06]\n",
      "Iter 10, loss [-0.045807872, -0.045812204, 4.334217e-06]\n",
      "Iter 11, loss [-0.049418606, -0.049424842, 6.2366903e-06]\n",
      "Iter 12, loss [-0.057399035, -0.057417013, 1.797907e-05]\n",
      "Iter 13, loss [-0.04753952, -0.047560964, 2.1441088e-05]\n",
      "Iter 14, loss [-0.05822264, -0.058277138, 5.4496322e-05]\n",
      "Iter 15, loss [-0.04661828, -0.046705093, 8.681564e-05]\n",
      "Iter 16, loss [-0.055999875, -0.056164008, 0.00016413274]\n",
      "Iter 17, loss [-0.054820772, -0.054873433, 5.266192e-05]\n",
      "Iter 18, loss [-0.05619314, -0.05634924, 0.00015610209]\n",
      "Iter 19, loss [-0.05247094, -0.052519605, 4.8662096e-05]\n",
      "Iter 20, loss [-0.26076308, -0.26084077, 7.770633e-05]\n",
      "Iter 21, loss [-0.055181, -0.055209186, 2.818383e-05]\n",
      "Iter 22, loss [-0.0564206, -0.056448914, 2.831419e-05]\n",
      "Iter 23, loss [-0.04631701, -0.04635205, 3.5041056e-05]\n",
      "Iter 24, loss [-0.058056388, -0.0580818, 2.5410696e-05]\n",
      "Iter 25, loss [-0.04531636, -0.04533306, 1.6699727e-05]\n",
      "Iter 26, loss [-0.05853898, -0.05856549, 2.6510028e-05]\n",
      "Iter 27, loss [-0.054827143, -0.054838844, 1.1702381e-05]\n",
      "Iter 28, loss [-0.040551435, -0.040561613, 1.0178341e-05]\n",
      "Iter 29, loss [-0.056985904, -0.056999184, 1.3282475e-05]\n",
      "Iter 30, loss [-0.044363145, -0.044373244, 1.0101062e-05]\n",
      "Iter 31, loss [-0.059251107, -0.059267037, 1.593104e-05]\n",
      "Iter 32, loss [-0.31277663, -0.31278956, 1.2927248e-05]\n",
      "Iter 33, loss [-0.053002886, -0.053009145, 6.257165e-06]\n",
      "Iter 34, loss [-0.05289473, -0.052900534, 5.803118e-06]\n",
      "Iter 35, loss [-0.05388653, -0.0538935, 6.968466e-06]\n",
      "Iter 36, loss [-0.04381634, -0.043823592, 7.25282e-06]\n",
      "Iter 37, loss [-0.06065406, -0.06066162, 7.5617036e-06]\n",
      "Iter 38, loss [-0.046222787, -0.046229184, 6.396567e-06]\n",
      "Iter 39, loss [-0.060643256, -0.060650717, 7.460707e-06]\n",
      "Iter 40, loss [-0.060665257, -0.0606728, 7.544474e-06]\n",
      "Iter 41, loss [-0.05630435, -0.056313086, 8.736183e-06]\n",
      "Iter 42, loss [-0.058607753, -0.058619518, 1.1762762e-05]\n",
      "Iter 43, loss [-0.04436679, -0.044377863, 1.1072329e-05]\n",
      "Iter 44, loss [-0.31439486, -0.31440583, 1.0952545e-05]\n",
      "Iter 45, loss [-0.04003635, -0.040043484, 7.134053e-06]\n",
      "Iter 46, loss [-0.056512043, -0.05652181, 9.766795e-06]\n",
      "Iter 47, loss [-0.052977398, -0.052983504, 6.1069786e-06]\n",
      "Iter 48, loss [-0.0464602, -0.04646782, 7.619742e-06]\n",
      "Iter 49, loss [-0.061002668, -0.06101167, 9.001047e-06]\n",
      "Iter 50, loss [-0.3140733, -0.31408453, 1.1236454e-05]\n",
      "Iter 51, loss [-0.054125477, -0.054133248, 7.772498e-06]\n",
      "Iter 52, loss [-0.049677126, -0.049683843, 6.7178416e-06]\n",
      "Iter 53, loss [-0.046455372, -0.046462703, 7.331944e-06]\n",
      "Iter 54, loss [-0.060943764, -0.060952064, 8.298997e-06]\n",
      "Iter 55, loss [-0.053017635, -0.05302376, 6.124067e-06]\n",
      "Iter 56, loss [-0.054135792, -0.054143436, 7.642651e-06]\n",
      "Iter 57, loss [-0.31455675, -0.31456718, 1.0426289e-05]\n",
      "Iter 58, loss [-0.058840305, -0.058852576, 1.22729525e-05]\n",
      "Iter 59, loss [-0.057350304, -0.057362802, 1.2496824e-05]\n",
      "Iter 60, loss [-0.04409315, -0.044101067, 7.916509e-06]\n",
      "Iter 61, loss [-0.044123553, -0.044131577, 8.02523e-06]\n",
      "Iter 62, loss [-0.044179678, -0.044187937, 8.258528e-06]\n",
      "Iter 63, loss [-0.04016312, -0.04017045, 7.329899e-06]\n",
      "Iter 64, loss [-0.054379474, -0.054387923, 8.4501535e-06]\n",
      "Iter 65, loss [-0.050119705, -0.050127946, 8.240213e-06]\n",
      "Iter 66, loss [-0.057951234, -0.057967503, 1.6268532e-05]\n",
      "Iter 67, loss [-0.061577573, -0.06158911, 1.1538457e-05]\n",
      "Iter 68, loss [-0.059923127, -0.05994179, 1.8664912e-05]\n",
      "Iter 69, loss [-0.058566406, -0.058588248, 2.1840735e-05]\n",
      "Iter 70, loss [-0.054211326, -0.05422401, 1.2683166e-05]\n",
      "Iter 71, loss [-0.30526805, -0.30528823, 2.0164876e-05]\n",
      "Iter 72, loss [-0.058930337, -0.058956705, 2.6366737e-05]\n",
      "Iter 73, loss [-0.30722663, -0.30724463, 1.7994942e-05]\n",
      "Iter 74, loss [-0.045152735, -0.045166228, 1.3493353e-05]\n",
      "Iter 75, loss [-0.053777006, -0.053786404, 9.40012e-06]\n",
      "Iter 76, loss [-0.31310993, -0.31312153, 1.1583855e-05]\n",
      "Iter 77, loss [-0.04019045, -0.040197782, 7.3323617e-06]\n",
      "Iter 78, loss [-0.03995519, -0.03996144, 6.246347e-06]\n",
      "Iter 79, loss [-0.05385176, -0.053857785, 6.023461e-06]\n",
      "Iter 80, loss [-0.0560363, -0.056042936, 6.636508e-06]\n",
      "Iter 81, loss [-0.03952482, -0.039529387, 4.5665747e-06]\n",
      "Iter 82, loss [-0.055817906, -0.055823714, 5.806196e-06]\n",
      "Iter 83, loss [-0.046096545, -0.04610113, 4.587235e-06]\n",
      "Iter 84, loss [-0.31895518, -0.31896028, 5.0870667e-06]\n",
      "Iter 85, loss [-0.04364825, -0.04365466, 6.4105116e-06]\n",
      "Iter 86, loss [-0.31944543, -0.31944993, 4.4974745e-06]\n",
      "Iter 87, loss [-0.039149784, -0.039153166, 3.380966e-06]\n",
      "Iter 88, loss [-0.043396775, -0.043402147, 5.3723006e-06]\n",
      "Iter 89, loss [-0.05529643, -0.05530052, 4.089399e-06]\n",
      "Iter 90, loss [-0.0529311, -0.052934404, 3.3061715e-06]\n",
      "Iter 91, loss [-0.048752926, -0.048756015, 3.0896642e-06]\n",
      "Iter 92, loss [-0.052948568, -0.05295191, 3.341353e-06]\n",
      "Iter 93, loss [-0.043047525, -0.0430513, 3.772784e-06]\n",
      "Iter 94, loss [-0.043411627, -0.043417074, 5.4471475e-06]\n",
      "Iter 95, loss [-0.045923963, -0.04592778, 3.819013e-06]\n",
      "Iter 96, loss [-0.31975526, -0.3197593, 4.0585846e-06]\n",
      "Iter 97, loss [-0.046008594, -0.04601272, 4.124033e-06]\n",
      "Iter 98, loss [-0.049121372, -0.04912536, 3.9848255e-06]\n",
      "Iter 99, loss [-0.05559089, -0.0555959, 5.0093868e-06]\n",
      "Iter 100, loss [-0.05566333, -0.055668578, 5.2489354e-06]\n",
      "Iter 101, loss [-0.03942181, -0.0394261, 4.2868514e-06]\n",
      "Iter 102, loss [-0.0395128, -0.0395174, 4.6020905e-06]\n",
      "Iter 103, loss [-0.04383747, -0.04384381, 6.339451e-06]\n",
      "Iter 104, loss [-0.049820926, -0.0498271, 6.172642e-06]\n",
      "Iter 105, loss [-0.05336759, -0.05337434, 6.748794e-06]\n",
      "Iter 106, loss [-0.061065853, -0.061073326, 7.4746144e-06]\n",
      "Iter 107, loss [-0.050476544, -0.050485536, 8.99197e-06]\n",
      "Iter 108, loss [-0.05074931, -0.050759777, 1.0467327e-05]\n",
      "Iter 109, loss [-0.057185855, -0.057198096, 1.2242042e-05]\n",
      "Iter 110, loss [-0.051369693, -0.051384497, 1.480374e-05]\n",
      "Iter 111, loss [-0.31056455, -0.31057724, 1.269708e-05]\n",
      "Iter 112, loss [-0.05773469, -0.05775085, 1.6161684e-05]\n",
      "Iter 113, loss [-0.04595742, -0.045976993, 1.9572206e-05]\n",
      "Iter 114, loss [-0.055005547, -0.05502576, 2.0213263e-05]\n",
      "Iter 115, loss [-0.046082728, -0.046107642, 2.4916522e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 116, loss [-0.04621752, -0.04624444, 2.69207e-05]\n",
      "Iter 117, loss [-0.060568064, -0.06059159, 2.3525685e-05]\n",
      "Iter 118, loss [-0.046584573, -0.04661817, 3.3598586e-05]\n",
      "Iter 119, loss [-0.06109863, -0.06112726, 2.8630157e-05]\n",
      "Iter 120, loss [-0.047978435, -0.048019946, 4.1509276e-05]\n",
      "Iter 121, loss [-0.047117334, -0.047169417, 5.2081956e-05]\n",
      "Iter 122, loss [-0.047380626, -0.047438182, 5.755618e-05]\n",
      "Iter 123, loss [-0.056577254, -0.05663699, 5.9735277e-05]\n",
      "Iter 124, loss [-0.28892642, -0.28895748, 3.1066942e-05]\n",
      "Iter 125, loss [-0.047978364, -0.048024587, 4.6223864e-05]\n",
      "Iter 126, loss [-0.059170175, -0.05919836, 2.8185277e-05]\n",
      "Iter 127, loss [-0.31015456, -0.3101682, 1.3643874e-05]\n",
      "Iter 128, loss [-0.3139162, -0.3139265, 1.0274991e-05]\n",
      "Iter 129, loss [-0.05092862, -0.050940596, 1.1978509e-05]\n",
      "Iter 130, loss [-0.056865968, -0.056877185, 1.1215753e-05]\n",
      "Iter 131, loss [-0.039417822, -0.039422836, 5.01239e-06]\n",
      "Iter 132, loss [-0.03916646, -0.039170597, 4.134994e-06]\n",
      "Iter 133, loss [-0.055262458, -0.055266995, 4.538138e-06]\n",
      "Iter 134, loss [-0.038841456, -0.038844585, 3.1279712e-06]\n",
      "Iter 135, loss [-0.32114032, -0.32114288, 2.569517e-06]\n",
      "Iter 136, loss [-0.055306107, -0.05531148, 5.3735725e-06]\n",
      "Iter 137, loss [-0.048353598, -0.04835601, 2.4138708e-06]\n",
      "Iter 138, loss [-0.042441454, -0.04244406, 2.608786e-06]\n",
      "Iter 139, loss [-0.048191212, -0.04819331, 2.0957593e-06]\n",
      "Iter 140, loss [-0.05222453, -0.052226633, 2.1022404e-06]\n",
      "Iter 141, loss [-0.042331554, -0.04233391, 2.354292e-06]\n",
      "Iter 142, loss [-0.05628985, -0.056294385, 4.535268e-06]\n",
      "Iter 143, loss [-0.04234496, -0.042347353, 2.3917105e-06]\n",
      "Iter 144, loss [-0.05467395, -0.054677002, 3.0506008e-06]\n",
      "Iter 145, loss [-0.32141057, -0.3214128, 2.2205754e-06]\n",
      "Iter 146, loss [-0.05159108, -0.05159311, 2.0307252e-06]\n",
      "Iter 147, loss [-0.042807993, -0.042812176, 4.1827434e-06]\n",
      "Iter 148, loss [-0.042493857, -0.042496648, 2.7896601e-06]\n",
      "Iter 149, loss [-0.055682752, -0.055689838, 7.0860724e-06]\n",
      "Iter 150, loss [-0.04261174, -0.04261487, 3.1283253e-06]\n",
      "Iter 151, loss [-0.045563765, -0.045566827, 3.064045e-06]\n",
      "Iter 152, loss [-0.04277849, -0.042782135, 3.6461147e-06]\n",
      "Iter 153, loss [-0.045700114, -0.0457037, 3.5870066e-06]\n",
      "Iter 154, loss [-0.056628175, -0.05663986, 1.1685816e-05]\n",
      "Iter 155, loss [-0.043117613, -0.04312246, 4.847584e-06]\n",
      "Iter 156, loss [-0.055884033, -0.055891965, 7.930643e-06]\n",
      "Iter 157, loss [-0.31800854, -0.31801504, 6.501662e-06]\n",
      "Iter 158, loss [-0.056150436, -0.056159854, 9.417608e-06]\n",
      "Iter 159, loss [-0.05247766, -0.052482124, 4.462393e-06]\n",
      "Iter 160, loss [-0.046247642, -0.046253927, 6.285817e-06]\n",
      "Iter 161, loss [-0.057929475, -0.057951383, 2.1909997e-05]\n",
      "Iter 162, loss [-0.039881382, -0.039890856, 9.4727075e-06]\n",
      "Iter 163, loss [-0.31561476, -0.3156242, 9.434001e-06]\n",
      "Iter 164, loss [-0.0528401, -0.052845962, 5.864852e-06]\n",
      "Iter 165, loss [-0.04443945, -0.044454537, 1.5085584e-05]\n",
      "Iter 166, loss [-0.31627062, -0.31627938, 8.77109e-06]\n",
      "Iter 167, loss [-0.31703067, -0.31703854, 7.873363e-06]\n",
      "Iter 168, loss [-0.039651122, -0.039659232, 8.110029e-06]\n",
      "Iter 169, loss [-0.043556586, -0.043563306, 6.718611e-06]\n",
      "Iter 170, loss [-0.052559722, -0.05256453, 4.8101e-06]\n",
      "Iter 171, loss [-0.060423486, -0.060429785, 6.299777e-06]\n",
      "Iter 172, loss [-0.039308473, -0.039314438, 5.9649747e-06]\n",
      "Iter 173, loss [-0.04612684, -0.046132352, 5.5133087e-06]\n",
      "Iter 174, loss [-0.05673033, -0.056743566, 1.32367095e-05]\n",
      "Iter 175, loss [-0.05675423, -0.05676757, 1.33414505e-05]\n",
      "Iter 176, loss [-0.04622734, -0.04623335, 6.0080556e-06]\n",
      "Iter 177, loss [-0.055895787, -0.055903524, 7.736958e-06]\n",
      "Iter 178, loss [-0.046361953, -0.046368685, 6.7299234e-06]\n",
      "Iter 179, loss [-0.05373075, -0.05373739, 6.6419393e-06]\n",
      "Iter 180, loss [-0.044185232, -0.044196982, 1.17503205e-05]\n",
      "Iter 181, loss [-0.05636861, -0.056378596, 9.985192e-06]\n",
      "Iter 182, loss [-0.059029058, -0.059047364, 1.8304381e-05]\n",
      "Iter 183, loss [-0.050203513, -0.050212834, 9.320774e-06]\n",
      "Iter 184, loss [-0.0402613, -0.04027231, 1.1012948e-05]\n",
      "Iter 185, loss [-0.05988931, -0.059914242, 2.4934343e-05]\n",
      "Iter 186, loss [-0.050862275, -0.050875768, 1.3493975e-05]\n",
      "Iter 187, loss [-0.05770569, -0.057725333, 1.9642968e-05]\n",
      "Iter 188, loss [-0.047757432, -0.047777355, 1.9921681e-05]\n",
      "Iter 189, loss [-0.061311804, -0.061354954, 4.315014e-05]\n",
      "Iter 190, loss [-0.062947825, -0.06297435, 2.652346e-05]\n",
      "Iter 191, loss [-0.058865927, -0.05890262, 3.6692414e-05]\n",
      "Iter 192, loss [-0.042241063, -0.042275883, 3.4819503e-05]\n",
      "Iter 193, loss [-0.29403728, -0.29407457, 3.7278085e-05]\n",
      "Iter 194, loss [-0.05918506, -0.05923097, 4.5908157e-05]\n",
      "Iter 195, loss [-0.06334112, -0.06337665, 3.5533158e-05]\n",
      "Iter 196, loss [-0.055265743, -0.05529712, 3.1378582e-05]\n",
      "Iter 197, loss [-0.30536488, -0.3053882, 2.332365e-05]\n",
      "Iter 198, loss [-0.05617674, -0.056202225, 2.548446e-05]\n",
      "Iter 199, loss [-0.045913193, -0.045939323, 2.6130894e-05]\n",
      "Iter 200, loss [-0.055701673, -0.05572215, 2.0477994e-05]\n",
      "Iter 201, loss [-0.05555498, -0.055574227, 1.9247424e-05]\n",
      "Iter 202, loss [-0.04544784, -0.04546982, 2.1978958e-05]\n",
      "Iter 203, loss [-0.04764095, -0.047661588, 2.0639962e-05]\n",
      "Iter 204, loss [-0.060218226, -0.060256645, 3.8419214e-05]\n",
      "Iter 205, loss [-0.054638203, -0.054658126, 1.9922685e-05]\n",
      "Iter 206, loss [-0.051413488, -0.05143353, 2.004209e-05]\n",
      "Iter 207, loss [-0.045549173, -0.0455815, 3.232665e-05]\n",
      "Iter 208, loss [-0.04567645, -0.04570337, 2.691904e-05]\n",
      "Iter 209, loss [-0.05587093, -0.055895004, 2.407236e-05]\n",
      "Iter 210, loss [-0.04600577, -0.046038184, 3.241397e-05]\n",
      "Iter 211, loss [-0.0625292, -0.062555075, 2.5875845e-05]\n",
      "Iter 212, loss [-0.046423048, -0.046463944, 4.089546e-05]\n",
      "Iter 213, loss [-0.056660276, -0.056698013, 3.773707e-05]\n",
      "Iter 214, loss [-0.04834695, -0.048397362, 5.041191e-05]\n",
      "Iter 215, loss [-0.061574843, -0.061646786, 7.1944276e-05]\n",
      "Iter 216, loss [-0.061785553, -0.061864484, 7.8931116e-05]\n",
      "Iter 217, loss [-0.05270597, -0.052778445, 7.2474184e-05]\n",
      "Iter 218, loss [-0.062173925, -0.06226971, 9.5784315e-05]\n",
      "Iter 219, loss [-0.05947179, -0.059543554, 7.176468e-05]\n",
      "Iter 220, loss [-0.047432076, -0.04755149, 0.0001194138]\n",
      "Iter 221, loss [-0.056884743, -0.056986853, 0.00010211093]\n",
      "Iter 222, loss [-0.2975756, -0.29761177, 3.6178535e-05]\n",
      "Iter 223, loss [-0.0475763, -0.047691703, 0.000115400435]\n",
      "Iter 224, loss [-0.047557943, -0.04765832, 0.00010037779]\n",
      "Iter 225, loss [-0.052697472, -0.0527769, 7.9428384e-05]\n",
      "Iter 226, loss [-0.047104213, -0.04719082, 8.660557e-05]\n",
      "Iter 227, loss [-0.059863206, -0.059955414, 9.220943e-05]\n",
      "Iter 228, loss [-0.058807097, -0.058858518, 5.141869e-05]\n",
      "Iter 229, loss [-0.058718003, -0.058767457, 4.9453804e-05]\n",
      "Iter 230, loss [-0.04166121, -0.041704718, 4.350699e-05]\n",
      "Iter 231, loss [-0.052719332, -0.052770797, 5.1466508e-05]\n",
      "Iter 232, loss [-0.04668327, -0.046752155, 6.8885965e-05]\n",
      "Iter 233, loss [-0.059744008, -0.05983356, 8.955285e-05]\n",
      "Iter 234, loss [-0.06315504, -0.063195094, 4.0051862e-05]\n",
      "Iter 235, loss [-0.04847071, -0.048529826, 5.911739e-05]\n",
      "Iter 236, loss [-0.05555874, -0.05562024, 6.149532e-05]\n",
      "Iter 237, loss [-0.041840117, -0.041891098, 5.097998e-05]\n",
      "Iter 238, loss [-0.041902136, -0.041955307, 5.3169642e-05]\n",
      "Iter 239, loss [-0.05287079, -0.05293234, 6.1551094e-05]\n",
      "Iter 240, loss [-0.062104866, -0.062217865, 0.00011299966]\n",
      "Iter 241, loss [-0.3091578, -0.30917823, 2.0446567e-05]\n",
      "Iter 242, loss [-0.31019476, -0.3102138, 1.9048966e-05]\n",
      "Iter 243, loss [-0.05281969, -0.052875977, 5.628355e-05]\n",
      "Iter 244, loss [-0.05860713, -0.058657985, 5.0852912e-05]\n",
      "Iter 245, loss [-0.06284122, -0.06287737, 3.615017e-05]\n",
      "Iter 246, loss [-0.046276268, -0.04633831, 6.203962e-05]\n",
      "Iter 247, loss [-0.3165293, -0.31653982, 1.0516103e-05]\n",
      "Iter 248, loss [-0.3174241, -0.31743345, 9.350269e-06]\n",
      "Iter 249, loss [-0.04597646, -0.04601893, 4.246992e-05]\n",
      "Iter 250, loss [-0.055882905, -0.055913534, 3.0627485e-05]\n",
      "Iter 251, loss [-0.05820596, -0.058261912, 5.5955432e-05]\n",
      "Iter 252, loss [-0.045168255, -0.045210447, 4.2193857e-05]\n",
      "Iter 253, loss [-0.0518138, -0.05184547, 3.166774e-05]\n",
      "Iter 254, loss [-0.055192158, -0.05522738, 3.5223926e-05]\n",
      "Iter 255, loss [-0.059084382, -0.05913195, 4.7566864e-05]\n",
      "Iter 256, loss [-0.061368816, -0.061389834, 2.1019141e-05]\n",
      "Iter 257, loss [-0.055368323, -0.055411175, 4.2851923e-05]\n",
      "Iter 258, loss [-0.040349524, -0.040379252, 2.9726292e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 259, loss [-0.048068643, -0.048119143, 5.0501763e-05]\n",
      "Iter 260, loss [-0.05958553, -0.059645236, 5.9706406e-05]\n",
      "Iter 261, loss [-0.05980933, -0.05987408, 6.474943e-05]\n",
      "Iter 262, loss [-0.056751803, -0.056807008, 5.5206674e-05]\n",
      "Iter 263, loss [-0.06232899, -0.06236475, 3.575731e-05]\n",
      "Iter 264, loss [-0.05916359, -0.059256513, 9.29229e-05]\n",
      "Iter 265, loss [-0.057143494, -0.057223946, 8.045159e-05]\n",
      "Iter 266, loss [-0.057165742, -0.057256345, 9.060455e-05]\n",
      "Iter 267, loss [-0.047058992, -0.047188424, 0.00012943053]\n",
      "Iter 268, loss [-0.052920442, -0.05306864, 0.00014819884]\n",
      "Iter 269, loss [-0.06347386, -0.063538566, 6.470579e-05]\n",
      "Iter 270, loss [-0.04731286, -0.047458913, 0.00014605394]\n",
      "Iter 271, loss [-0.053350907, -0.053493425, 0.00014251695]\n",
      "Iter 272, loss [-0.062224057, -0.06239061, 0.00016655444]\n",
      "Iter 273, loss [-0.062256392, -0.06243088, 0.0001744881]\n",
      "Iter 274, loss [-0.31257355, -0.31259325, 1.9692707e-05]\n",
      "Iter 275, loss [-0.04731792, -0.04744891, 0.00013099238]\n",
      "Iter 276, loss [-0.047223575, -0.047347642, 0.00012406704]\n",
      "Iter 277, loss [-0.06353068, -0.06359772, 6.703247e-05]\n",
      "Iter 278, loss [-0.047120385, -0.04723916, 0.00011877419]\n",
      "Iter 279, loss [-0.06352201, -0.063590385, 6.8372734e-05]\n",
      "Iter 280, loss [-0.05927264, -0.05936678, 9.4141804e-05]\n",
      "Iter 281, loss [-0.057472438, -0.057542242, 6.980587e-05]\n",
      "Iter 282, loss [-0.063720845, -0.06379952, 7.8678815e-05]\n",
      "Iter 283, loss [-0.05310933, -0.05318559, 7.626178e-05]\n",
      "Iter 284, loss [-0.059669394, -0.059788033, 0.000118641125]\n",
      "Iter 285, loss [-0.047506843, -0.047668424, 0.00016158145]\n",
      "Iter 286, loss [-0.0599209, -0.060059004, 0.00013810273]\n",
      "Iter 287, loss [-0.05894991, -0.059275877, 0.00032596718]\n",
      "Iter 288, loss [-0.06204806, -0.06235657, 0.00030850846]\n",
      "Iter 289, loss [-0.062095225, -0.062404804, 0.00030957983]\n",
      "Iter 290, loss [-0.057791974, -0.05790539, 0.00011341724]\n",
      "Iter 291, loss [-0.047921963, -0.04806956, 0.00014759664]\n",
      "Iter 292, loss [-0.05293719, -0.053057488, 0.000120297474]\n",
      "Iter 293, loss [-0.062346242, -0.062639974, 0.00029373352]\n",
      "Iter 294, loss [-0.057815358, -0.05793266, 0.00011730367]\n",
      "Iter 295, loss [-0.047817126, -0.048018225, 0.00020109816]\n",
      "Iter 296, loss [-0.0530904, -0.053205367, 0.000114966846]\n",
      "Iter 297, loss [-0.042816762, -0.042959522, 0.00014275877]\n",
      "Iter 298, loss [-0.06016568, -0.0603084, 0.00014271849]\n",
      "Iter 299, loss [-0.06433854, -0.0644496, 0.00011105735]\n",
      "Iter 300, loss [-0.048027486, -0.04816558, 0.00013809334]\n",
      "Iter 301, loss [-0.060247432, -0.06039427, 0.00014683619]\n",
      "Iter 302, loss [-0.048757676, -0.048889022, 0.00013134512]\n",
      "Iter 303, loss [-0.060325857, -0.060479116, 0.0001532573]\n",
      "Iter 304, loss [-0.06259428, -0.0628841, 0.00028981688]\n",
      "Iter 305, loss [-0.04813849, -0.0482869, 0.000148408]\n",
      "Iter 306, loss [-0.04303407, -0.043193337, 0.00015926552]\n",
      "Iter 307, loss [-0.048023295, -0.048242293, 0.00021899823]\n",
      "Iter 308, loss [-0.048042692, -0.048266474, 0.00022378376]\n",
      "Iter 309, loss [-0.055481784, -0.055611987, 0.00013020399]\n",
      "Iter 310, loss [-0.048238743, -0.048395965, 0.00015722224]\n",
      "Iter 311, loss [-0.043128278, -0.043297168, 0.00016888864]\n",
      "Iter 312, loss [-0.06462395, -0.064758755, 0.000134801]\n",
      "Iter 313, loss [-0.04314745, -0.04331985, 0.00017240175]\n",
      "Iter 314, loss [-0.04316771, -0.04334329, 0.00017558032]\n",
      "Iter 315, loss [-0.06250719, -0.06284946, 0.0003422706]\n",
      "Iter 316, loss [-0.048169635, -0.048404854, 0.0002352175]\n",
      "Iter 317, loss [-0.060660325, -0.06084336, 0.00018303648]\n",
      "Iter 318, loss [-0.05601084, -0.05613278, 0.000121941455]\n",
      "Iter 319, loss [-0.059151456, -0.059523556, 0.00037210173]\n",
      "Iter 320, loss [-0.048391048, -0.048542112, 0.00015106332]\n",
      "Iter 321, loss [-0.06274581, -0.063065685, 0.0003198726]\n",
      "Iter 322, loss [-0.06468229, -0.06480935, 0.00012706185]\n",
      "Iter 323, loss [-0.043167524, -0.043322265, 0.0001547409]\n",
      "Iter 324, loss [-0.064666755, -0.064787865, 0.00012110948]\n",
      "Iter 325, loss [-0.30412465, -0.30415976, 3.5097084e-05]\n",
      "Iter 326, loss [-0.06450362, -0.06460765, 0.00010403257]\n",
      "Iter 327, loss [-0.062896535, -0.06312084, 0.0002243068]\n",
      "Iter 328, loss [-0.05981474, -0.05992466, 0.000109919834]\n",
      "Iter 329, loss [-0.04225116, -0.042351414, 0.00010025373]\n",
      "Iter 330, loss [-0.048810035, -0.048890658, 8.062398e-05]\n",
      "Iter 331, loss [-0.062391654, -0.0625706, 0.00017894681]\n",
      "Iter 332, loss [-0.06364945, -0.06372081, 7.135417e-05]\n",
      "Iter 333, loss [-0.041998815, -0.042092286, 9.3471055e-05]\n",
      "Iter 334, loss [-0.059365153, -0.059463892, 9.873755e-05]\n",
      "Iter 335, loss [-0.05946513, -0.059568413, 0.00010328418]\n",
      "Iter 336, loss [-0.06055051, -0.06076513, 0.00021461716]\n",
      "Iter 337, loss [-0.06410417, -0.064191654, 8.748666e-05]\n",
      "Iter 338, loss [-0.06279873, -0.06302109, 0.00022235961]\n",
      "Iter 339, loss [-0.04933596, -0.049462125, 0.00012616324]\n",
      "Iter 340, loss [-0.06025027, -0.060396947, 0.00014667661]\n",
      "Iter 341, loss [-0.05830592, -0.05843743, 0.00013150876]\n",
      "Iter 342, loss [-0.043039154, -0.043207835, 0.00016868231]\n",
      "Iter 343, loss [-0.048524596, -0.04872966, 0.00020506517]\n",
      "Iter 344, loss [-0.06022967, -0.06059591, 0.00036624083]\n",
      "Iter 345, loss [-0.048383545, -0.048698463, 0.0003149188]\n",
      "Iter 346, loss [-0.0631698, -0.06354572, 0.00037592056]\n",
      "Iter 347, loss [-0.048920576, -0.049179982, 0.00025940535]\n",
      "Iter 348, loss [-0.06010591, -0.060507778, 0.0004018686]\n",
      "Iter 349, loss [-0.065177284, -0.06536139, 0.00018410035]\n",
      "Iter 350, loss [-0.04860907, -0.04889165, 0.00028257698]\n",
      "Iter 351, loss [-0.053563487, -0.05378928, 0.000225794]\n",
      "Iter 352, loss [-0.05855063, -0.058760308, 0.00020967823]\n",
      "Iter 353, loss [-0.04880393, -0.04905075, 0.0002468178]\n",
      "Iter 354, loss [-0.056291558, -0.056478694, 0.000187136]\n",
      "Iter 355, loss [-0.048785787, -0.0490039, 0.00021811284]\n",
      "Iter 356, loss [-0.060933813, -0.061272524, 0.00033870942]\n",
      "Iter 357, loss [-0.048623048, -0.04882153, 0.00019848235]\n",
      "Iter 358, loss [-0.308731, -0.30876943, 3.8456237e-05]\n",
      "Iter 359, loss [-0.31142318, -0.3114568, 3.361463e-05]\n",
      "Iter 360, loss [-0.05986531, -0.060008988, 0.00014367604]\n",
      "Iter 361, loss [-0.05432961, -0.054397482, 6.786949e-05]\n",
      "Iter 362, loss [-0.04585292, -0.0459508, 9.788088e-05]\n",
      "Iter 363, loss [-0.05400878, -0.054064743, 5.5964945e-05]\n",
      "Iter 364, loss [-0.041010745, -0.041102313, 9.156625e-05]\n",
      "Iter 365, loss [-0.062450826, -0.062515266, 6.44409e-05]\n",
      "Iter 366, loss [-0.047893915, -0.047974464, 8.0549304e-05]\n",
      "Iter 367, loss [-0.060837384, -0.060980223, 0.00014283745]\n",
      "Iter 368, loss [-0.04610026, -0.046193473, 9.321397e-05]\n",
      "Iter 369, loss [-0.046325654, -0.046423376, 9.772248e-05]\n",
      "Iter 370, loss [-0.3196639, -0.3196773, 1.3374802e-05]\n",
      "Iter 371, loss [-0.058463212, -0.058556855, 9.364216e-05]\n",
      "Iter 372, loss [-0.04653466, -0.046677597, 0.00014293553]\n",
      "Iter 373, loss [-0.04668128, -0.04683225, 0.00015096579]\n",
      "Iter 374, loss [-0.057395086, -0.057492897, 9.781118e-05]\n",
      "Iter 375, loss [-0.06034815, -0.060558263, 0.00021011286]\n",
      "Iter 376, loss [-0.062083196, -0.062297482, 0.00021428586]\n",
      "Iter 377, loss [-0.06400663, -0.06411392, 0.00010729382]\n",
      "Iter 378, loss [-0.048256036, -0.048449617, 0.00019358081]\n",
      "Iter 379, loss [-0.31588542, -0.3159127, 2.7278016e-05]\n",
      "Iter 380, loss [-0.04813298, -0.048424575, 0.00029159605]\n",
      "Iter 381, loss [-0.056375686, -0.056570735, 0.00019505042]\n",
      "Iter 382, loss [-0.048284188, -0.048609085, 0.00032489872]\n",
      "Iter 383, loss [-0.06317799, -0.06353467, 0.00035668138]\n",
      "Iter 384, loss [-0.314897, -0.31493124, 3.4252735e-05]\n",
      "Iter 385, loss [-0.05411092, -0.054336358, 0.00022544048]\n",
      "Iter 386, loss [-0.048380293, -0.048736528, 0.0003562332]\n",
      "Iter 387, loss [-0.048818875, -0.04911539, 0.0002965128]\n",
      "Iter 388, loss [-0.0428206, -0.043044314, 0.00022371326]\n",
      "Iter 389, loss [-0.31725731, -0.31728908, 3.178404e-05]\n",
      "Iter 390, loss [-0.05646021, -0.056711163, 0.00025095406]\n",
      "Iter 391, loss [-0.048187207, -0.04855094, 0.00036373368]\n",
      "Iter 392, loss [-0.060033374, -0.060258962, 0.00022558942]\n",
      "Iter 393, loss [-0.056661744, -0.05690435, 0.00024260447]\n",
      "Iter 394, loss [-0.048071995, -0.048439313, 0.00036731653]\n",
      "Iter 395, loss [-0.048114516, -0.04849086, 0.0003763438]\n",
      "Iter 396, loss [-0.059015706, -0.059263285, 0.0002475797]\n",
      "Iter 397, loss [-0.04264232, -0.042899996, 0.00025767574]\n",
      "Iter 398, loss [-0.06115155, -0.061607607, 0.00045605924]\n",
      "Iter 399, loss [-0.050073445, -0.05043181, 0.00035836396]\n",
      "Iter 400, loss [-0.31827286, -0.31831193, 3.907113e-05]\n",
      "Iter 401, loss [-0.06509259, -0.06533912, 0.00024652568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 402, loss [-0.04947669, -0.049913883, 0.00043719186]\n",
      "Iter 403, loss [-0.05680263, -0.05716449, 0.00036186035]\n",
      "Iter 404, loss [-0.04323531, -0.043605972, 0.00037066377]\n",
      "Iter 405, loss [-0.050388932, -0.050827604, 0.00043867325]\n",
      "Iter 406, loss [-0.061206967, -0.06159549, 0.00038852356]\n",
      "Iter 407, loss [-0.055109043, -0.055522352, 0.00041330897]\n",
      "Iter 408, loss [-0.050670475, -0.051170625, 0.0005001488]\n",
      "Iter 409, loss [-0.04953502, -0.05026716, 0.0007321404]\n",
      "Iter 410, loss [-0.049671, -0.050454956, 0.00078395556]\n",
      "Iter 411, loss [-0.050285663, -0.050961163, 0.00067549845]\n",
      "Iter 412, loss [-0.061389558, -0.062272068, 0.00088251167]\n",
      "Iter 413, loss [-0.05567187, -0.056346394, 0.00067452376]\n",
      "Iter 414, loss [-0.057314545, -0.058037024, 0.0007224788]\n",
      "Iter 415, loss [-0.06269222, -0.06334876, 0.0006565474]\n",
      "Iter 416, loss [-0.062797666, -0.06347706, 0.0006793991]\n",
      "Iter 417, loss [-0.064480804, -0.06554464, 0.0010638412]\n",
      "Iter 418, loss [-0.06173121, -0.06288018, 0.0011489701]\n",
      "Iter 419, loss [-0.051121928, -0.05208338, 0.00096145086]\n",
      "Iter 420, loss [-0.044417024, -0.045276232, 0.00085920934]\n",
      "Iter 421, loss [-0.051649038, -0.05284994, 0.0012009034]\n",
      "Iter 422, loss [-0.06273391, -0.06414157, 0.0014076587]\n",
      "Iter 423, loss [-0.0640586, -0.065201625, 0.0011430259]\n",
      "Iter 424, loss [-0.051770415, -0.053374086, 0.001603672]\n",
      "Iter 425, loss [-0.05695958, -0.058451563, 0.0014919811]\n",
      "Iter 426, loss [-0.06453067, -0.06582521, 0.0012945416]\n",
      "Iter 427, loss [-0.0647282, -0.06601871, 0.0012905087]\n",
      "Iter 428, loss [-0.068190575, -0.06924851, 0.0010579353]\n",
      "Iter 429, loss [-0.05215451, -0.054148346, 0.001993835]\n",
      "Iter 430, loss [-0.06680163, -0.06876594, 0.001964307]\n",
      "Iter 431, loss [-0.05258645, -0.054291725, 0.0017052729]\n",
      "Iter 432, loss [-0.045318484, -0.047067598, 0.0017491117]\n",
      "Iter 433, loss [-0.053497184, -0.05596758, 0.0024703974]\n",
      "Iter 434, loss [-0.067088194, -0.06968963, 0.0026014387]\n",
      "Iter 435, loss [-0.06601101, -0.06802232, 0.002011309]\n",
      "Iter 436, loss [-0.06631537, -0.06820422, 0.0018888467]\n",
      "Iter 437, loss [-0.05891147, -0.06074452, 0.0018330503]\n",
      "Iter 438, loss [-0.045996323, -0.047632854, 0.001636531]\n",
      "Iter 439, loss [-0.06949976, -0.070975415, 0.0014756515]\n",
      "Iter 440, loss [-0.055005662, -0.057439, 0.002433336]\n",
      "Iter 441, loss [-0.059887115, -0.062366363, 0.0024792482]\n",
      "Iter 442, loss [-0.0658763, -0.06825113, 0.002374837]\n",
      "Iter 443, loss [-0.060364798, -0.06280008, 0.0024352823]\n",
      "Iter 444, loss [-0.067691036, -0.070079304, 0.0023882657]\n",
      "Iter 445, loss [-0.067992866, -0.07045045, 0.0024575815]\n",
      "Iter 446, loss [-0.061554372, -0.06379449, 0.0022401218]\n",
      "Iter 447, loss [-0.06190844, -0.06380457, 0.0018961264]\n",
      "Iter 448, loss [-0.0688847, -0.07158957, 0.002704867]\n",
      "Iter 449, loss [-0.06177331, -0.06330386, 0.001530548]\n",
      "Iter 450, loss [-0.068969175, -0.07225458, 0.0032854108]\n",
      "Iter 451, loss [-0.06619568, -0.06963919, 0.003443513]\n",
      "Iter 452, loss [-0.07066359, -0.0731701, 0.0025065127]\n",
      "Iter 453, loss [-0.05639104, -0.059644464, 0.0032534234]\n",
      "Iter 454, loss [-0.055555303, -0.0597398, 0.0041844957]\n",
      "Iter 455, loss [-0.066876166, -0.0697505, 0.0028743367]\n",
      "Iter 456, loss [-0.057367593, -0.06075892, 0.0033913243]\n",
      "Iter 457, loss [-0.06892216, -0.07264344, 0.003721276]\n",
      "Iter 458, loss [-0.06302176, -0.06667839, 0.003656635]\n",
      "Iter 459, loss [-0.069063045, -0.07259185, 0.0035288064]\n",
      "Iter 460, loss [-0.057156138, -0.06150283, 0.00434669]\n",
      "Iter 461, loss [-0.29555604, -0.29685742, 0.0013013686]\n",
      "Iter 462, loss [-0.061022967, -0.06343403, 0.0024110598]\n",
      "Iter 463, loss [-0.056770954, -0.05940525, 0.0026342939]\n",
      "Iter 464, loss [-0.06181549, -0.06428066, 0.0024651687]\n",
      "Iter 465, loss [-0.06638655, -0.06856419, 0.0021776387]\n",
      "Iter 466, loss [-0.06565024, -0.068212435, 0.0025621941]\n",
      "Iter 467, loss [-0.055074133, -0.058681987, 0.003607854]\n",
      "Iter 468, loss [-0.070217274, -0.07206945, 0.0018521755]\n",
      "Iter 469, loss [-0.066548355, -0.06886556, 0.0023172055]\n",
      "Iter 470, loss [-0.31356677, -0.3140123, 0.0004455244]\n",
      "Iter 471, loss [-0.04592775, -0.047951203, 0.002023451]\n",
      "Iter 472, loss [-0.06675617, -0.06918966, 0.002433491]\n",
      "Iter 473, loss [-0.06577904, -0.06957646, 0.0037974173]\n",
      "Iter 474, loss [-0.058087632, -0.062465798, 0.004378168]\n",
      "Iter 475, loss [-0.067132115, -0.070759356, 0.0036272432]\n",
      "Iter 476, loss [-0.066282086, -0.06868409, 0.002402007]\n",
      "Iter 477, loss [-0.067523085, -0.07085231, 0.0033292263]\n",
      "Iter 478, loss [-0.061475124, -0.06491677, 0.0034416434]\n",
      "Iter 479, loss [-0.047193352, -0.04926118, 0.0020678262]\n",
      "Iter 480, loss [-0.06351052, -0.06699931, 0.003488786]\n",
      "Iter 481, loss [-0.06810197, -0.07142056, 0.0033185836]\n",
      "Iter 482, loss [-0.05551591, -0.060974333, 0.005458423]\n",
      "Iter 483, loss [-0.05782295, -0.06287029, 0.0050473376]\n",
      "Iter 484, loss [-0.05848369, -0.06359625, 0.0051125567]\n",
      "Iter 485, loss [-0.06418694, -0.06939889, 0.0052119484]\n",
      "Iter 486, loss [-0.069043964, -0.07291496, 0.0038709908]\n",
      "Iter 487, loss [-0.048556734, -0.0521456, 0.0035888678]\n",
      "Iter 488, loss [-0.3079159, -0.30860725, 0.0006913624]\n",
      "Iter 489, loss [-0.06470599, -0.06946179, 0.0047558006]\n",
      "Iter 490, loss [-0.07002894, -0.073793955, 0.003765015]\n",
      "Iter 491, loss [-0.056960814, -0.06315676, 0.0061959494]\n",
      "Iter 492, loss [-0.06483321, -0.06879869, 0.003965485]\n",
      "Iter 493, loss [-0.06907595, -0.072098956, 0.0030230067]\n",
      "Iter 494, loss [-0.047788378, -0.050241534, 0.0024531556]\n",
      "Iter 495, loss [-0.072474085, -0.0748294, 0.0023553134]\n",
      "Iter 496, loss [-0.07024278, -0.07343771, 0.0031949347]\n",
      "Iter 497, loss [-0.070132, -0.073960274, 0.0038282755]\n",
      "Iter 498, loss [-0.058353156, -0.06257228, 0.0042191236]\n",
      "Iter 499, loss [-0.074103, -0.07721142, 0.0031084174]\n",
      "Iter 500, loss [-0.0744651, -0.07798486, 0.0035197593]\n",
      "Iter 501, loss [-0.072144374, -0.0777768, 0.005632423]\n",
      "Iter 502, loss [-0.07175153, -0.07704377, 0.0052922415]\n",
      "Iter 503, loss [-0.058158472, -0.068047486, 0.009889016]\n",
      "Iter 504, loss [-0.07516846, -0.080219895, 0.0050514303]\n",
      "Iter 505, loss [-0.07330747, -0.079732746, 0.00642528]\n",
      "Iter 506, loss [-0.06774803, -0.07498805, 0.0072400193]\n",
      "Iter 507, loss [-0.073711134, -0.07996229, 0.0062511554]\n",
      "Iter 508, loss [-0.06151563, -0.06776466, 0.006249032]\n",
      "Iter 509, loss [-0.061667256, -0.06771828, 0.0060510277]\n",
      "Iter 510, loss [-0.061561637, -0.068124905, 0.006563268]\n",
      "Iter 511, loss [-0.07408654, -0.07942172, 0.0053351833]\n",
      "Iter 512, loss [-0.061722837, -0.06814147, 0.006418632]\n",
      "Iter 513, loss [-0.06017938, -0.06875271, 0.008573335]\n",
      "Iter 514, loss [-0.07672289, -0.08098485, 0.004261961]\n",
      "Iter 515, loss [-0.30130738, -0.3023967, 0.001089338]\n",
      "Iter 516, loss [-0.07347907, -0.078703806, 0.0052247345]\n",
      "Iter 517, loss [-0.075586855, -0.07908405, 0.003497201]\n",
      "Iter 518, loss [-0.07519446, -0.07859092, 0.003396458]\n",
      "Iter 519, loss [-0.060169738, -0.06573196, 0.00556222]\n",
      "Iter 520, loss [-0.06757269, -0.07265339, 0.005080701]\n",
      "Iter 521, loss [-0.068120316, -0.07393303, 0.0058127097]\n",
      "Iter 522, loss [-0.31399065, -0.31466842, 0.0006777663]\n",
      "Iter 523, loss [-0.07224864, -0.07710511, 0.0048564724]\n",
      "Iter 524, loss [-0.05861242, -0.06821399, 0.0096015725]\n",
      "Iter 525, loss [-0.060087442, -0.068888575, 0.008801134]\n",
      "Iter 526, loss [-0.06234074, -0.070521, 0.008180257]\n",
      "Iter 527, loss [-0.06659843, -0.07433764, 0.00773921]\n",
      "Iter 528, loss [-0.06715493, -0.07388961, 0.006734682]\n",
      "Iter 529, loss [-0.06743633, -0.07290891, 0.0054725795]\n",
      "Iter 530, loss [-0.07389371, -0.07894674, 0.005053029]\n",
      "Iter 531, loss [-0.06875515, -0.073418796, 0.0046636485]\n",
      "Iter 532, loss [-0.070806645, -0.07537409, 0.0045674415]\n",
      "Iter 533, loss [-0.057816707, -0.064347856, 0.0065311487]\n",
      "Iter 534, loss [-0.0755789, -0.0789572, 0.003378301]\n",
      "Iter 535, loss [-0.050235547, -0.054063242, 0.0038276955]\n",
      "Iter 536, loss [-0.07679352, -0.08069651, 0.0039029894]\n",
      "Iter 537, loss [-0.051212806, -0.055741213, 0.004528408]\n",
      "Iter 538, loss [-0.07441638, -0.08043252, 0.0060161455]\n",
      "Iter 539, loss [-0.07899648, -0.084294975, 0.005298499]\n",
      "Iter 540, loss [-0.06906938, -0.07685588, 0.007786504]\n",
      "Iter 541, loss [-0.069715805, -0.07685718, 0.007141377]\n",
      "Iter 542, loss [-0.07569301, -0.083562195, 0.007869184]\n",
      "Iter 543, loss [-0.0637055, -0.07362134, 0.009915844]\n",
      "Iter 544, loss [-0.079958394, -0.08679248, 0.006834085]\n",
      "Iter 545, loss [-0.06436697, -0.073001206, 0.008634236]\n",
      "Iter 546, loss [-0.07874352, -0.086939275, 0.00819576]\n",
      "Iter 547, loss [-0.07500818, -0.08371735, 0.008709169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 548, loss [-0.08150231, -0.087550744, 0.0060484363]\n",
      "Iter 549, loss [-0.07619701, -0.08429304, 0.008096021]\n",
      "Iter 550, loss [-0.2954778, -0.2966872, 0.0012093913]\n",
      "Iter 551, loss [-0.078365475, -0.0851086, 0.006743122]\n",
      "Iter 552, loss [-0.07299124, -0.08151415, 0.008522909]\n",
      "Iter 553, loss [-0.2936936, -0.29474905, 0.0010554439]\n",
      "Iter 554, loss [-0.07476813, -0.08019084, 0.0054227016]\n",
      "Iter 555, loss [-0.073968254, -0.079678684, 0.00571043]\n",
      "Iter 556, loss [-0.06257949, -0.06754559, 0.0049661016]\n",
      "Iter 557, loss [-0.06689932, -0.07098915, 0.004089823]\n",
      "Iter 558, loss [-0.07161385, -0.07535604, 0.0037421915]\n",
      "Iter 559, loss [-0.06038212, -0.06516476, 0.0047826404]\n",
      "Iter 560, loss [-0.07240985, -0.07615143, 0.0037415796]\n",
      "Iter 561, loss [-0.0663173, -0.071192235, 0.0048749344]\n",
      "Iter 562, loss [-0.06695212, -0.07205992, 0.005107805]\n",
      "Iter 563, loss [-0.07396431, -0.07937825, 0.0054139383]\n",
      "Iter 564, loss [-0.061846387, -0.06818708, 0.006340695]\n",
      "Iter 565, loss [-0.07381536, -0.07846372, 0.0046483586]\n",
      "Iter 566, loss [-0.07586445, -0.08095031, 0.0050858646]\n",
      "Iter 567, loss [-0.0763273, -0.08324693, 0.006919633]\n",
      "Iter 568, loss [-0.06807624, -0.0749927, 0.006916466]\n",
      "Iter 569, loss [-0.0629134, -0.0735273, 0.010613892]\n",
      "Iter 570, loss [-0.07844637, -0.085393995, 0.0069476226]\n",
      "Iter 571, loss [-0.07761419, -0.08450385, 0.006889662]\n",
      "Iter 572, loss [-0.07635489, -0.084333956, 0.007979063]\n",
      "Iter 573, loss [-0.06625031, -0.07708587, 0.010835561]\n",
      "Iter 574, loss [-0.08001068, -0.08879353, 0.008782852]\n",
      "Iter 575, loss [-0.078848585, -0.087338045, 0.008489456]\n",
      "Iter 576, loss [-0.066632494, -0.0771941, 0.0105616115]\n",
      "Iter 577, loss [-0.079473495, -0.088323675, 0.008850182]\n",
      "Iter 578, loss [-0.064362876, -0.07910955, 0.014746673]\n",
      "Iter 579, loss [-0.07213986, -0.0805075, 0.0083676465]\n",
      "Iter 580, loss [-0.06775234, -0.078018025, 0.010265686]\n",
      "Iter 581, loss [-0.065150574, -0.079144716, 0.013994139]\n",
      "Iter 582, loss [-0.06532808, -0.078991614, 0.013663536]\n",
      "Iter 583, loss [-0.08024136, -0.08860396, 0.008362598]\n",
      "Iter 584, loss [-0.056756843, -0.064154476, 0.0073976344]\n",
      "Iter 585, loss [-0.08130959, -0.09019264, 0.008883052]\n",
      "Iter 586, loss [-0.08095139, -0.08920538, 0.008253984]\n",
      "Iter 587, loss [-0.06863601, -0.0780676, 0.009431591]\n",
      "Iter 588, loss [-0.06917136, -0.078772254, 0.0096008945]\n",
      "Iter 589, loss [-0.06700758, -0.08075333, 0.01374575]\n",
      "Iter 590, loss [-0.067068025, -0.08100896, 0.013940934]\n",
      "Iter 591, loss [-0.06954666, -0.08053059, 0.010983932]\n",
      "Iter 592, loss [-0.08382157, -0.0941609, 0.010339327]\n",
      "Iter 593, loss [-0.08294944, -0.092151895, 0.00920246]\n",
      "Iter 594, loss [-0.295616, -0.29725426, 0.0016382623]\n",
      "Iter 595, loss [-0.084933415, -0.094917044, 0.009983633]\n",
      "Iter 596, loss [-0.08348323, -0.09243904, 0.008955816]\n",
      "Iter 597, loss [-0.07403587, -0.081369415, 0.00733355]\n",
      "Iter 598, loss [-0.083639555, -0.092766315, 0.009126761]\n",
      "Iter 599, loss [-0.08366652, -0.09213735, 0.008470832]\n",
      "Iter 600, loss [-0.07629511, -0.084506504, 0.008211396]\n",
      "Iter 601, loss [-0.057768323, -0.0651029, 0.007334574]\n",
      "Iter 602, loss [-0.07066489, -0.08005122, 0.009386333]\n",
      "Iter 603, loss [-0.08215275, -0.092590965, 0.010438217]\n",
      "Iter 604, loss [-0.07796114, -0.08761988, 0.009658739]\n",
      "Iter 605, loss [-0.08859342, -0.09713767, 0.008544248]\n",
      "Iter 606, loss [-0.05957707, -0.06886942, 0.009292349]\n",
      "Iter 607, loss [-0.06882145, -0.085390635, 0.01656918]\n",
      "Iter 608, loss [-0.0603641, -0.07046025, 0.010096151]\n",
      "Iter 609, loss [-0.08255545, -0.09596629, 0.013410838]\n",
      "Iter 610, loss [-0.07875171, -0.08904247, 0.010290756]\n",
      "Iter 611, loss [-0.08416714, -0.09598222, 0.011815078]\n",
      "Iter 612, loss [-0.07912128, -0.08968004, 0.0105587635]\n",
      "Iter 613, loss [-0.07257728, -0.08555403, 0.012976751]\n",
      "Iter 614, loss [-0.090491675, -0.10143573, 0.0109440535]\n",
      "Iter 615, loss [-0.2922837, -0.2944894, 0.002205697]\n",
      "Iter 616, loss [-0.085001275, -0.09731263, 0.012311352]\n",
      "Iter 617, loss [-0.08437362, -0.09615293, 0.011779312]\n",
      "Iter 618, loss [-0.07392463, -0.08378169, 0.009857057]\n",
      "Iter 619, loss [-0.08962643, -0.09889649, 0.009270057]\n",
      "Iter 620, loss [-0.07912721, -0.08826754, 0.009140337]\n",
      "Iter 621, loss [-0.07866236, -0.08738715, 0.008724794]\n",
      "Iter 622, loss [-0.078186534, -0.08660962, 0.0084230825]\n",
      "Iter 623, loss [-0.0712026, -0.08114456, 0.009941958]\n",
      "Iter 624, loss [-0.084233075, -0.09563461, 0.011401534]\n",
      "Iter 625, loss [-0.060200784, -0.06924415, 0.009043362]\n",
      "Iter 626, loss [-0.079690784, -0.08976869, 0.010077905]\n",
      "Iter 627, loss [-0.087264836, -0.100024074, 0.012759237]\n",
      "Iter 628, loss [-0.06100972, -0.07105856, 0.010048838]\n",
      "Iter 629, loss [-0.08800849, -0.10016997, 0.0121614775]\n",
      "Iter 630, loss [-0.070663646, -0.088893734, 0.018230092]\n",
      "Iter 631, loss [-0.081067786, -0.092362285, 0.011294495]\n",
      "Iter 632, loss [-0.29627034, -0.29846957, 0.002199241]\n",
      "Iter 633, loss [-0.08539824, -0.09945282, 0.014054581]\n",
      "Iter 634, loss [-0.089220546, -0.10114652, 0.011925973]\n",
      "Iter 635, loss [-0.08851336, -0.10128367, 0.012770313]\n",
      "Iter 636, loss [-0.08634705, -0.09716976, 0.010822709]\n",
      "Iter 637, loss [-0.30261838, -0.30440855, 0.0017901682]\n",
      "Iter 638, loss [-0.088902876, -0.100506455, 0.011603579]\n",
      "Iter 639, loss [-0.07352319, -0.08561507, 0.012091873]\n",
      "Iter 640, loss [-0.07316096, -0.08411284, 0.010951873]\n",
      "Iter 641, loss [-0.08054519, -0.09011392, 0.009568737]\n",
      "Iter 642, loss [-0.08951425, -0.09790327, 0.008389018]\n",
      "Iter 643, loss [-0.086186446, -0.09593107, 0.00974462]\n",
      "Iter 644, loss [-0.081619084, -0.09211784, 0.010498753]\n",
      "Iter 645, loss [-0.08114391, -0.09114082, 0.009996914]\n",
      "Iter 646, loss [-0.074671514, -0.08755345, 0.012881935]\n",
      "Iter 647, loss [-0.06289732, -0.07337587, 0.010478554]\n",
      "Iter 648, loss [-0.08642604, -0.10043123, 0.014005191]\n",
      "Iter 649, loss [-0.093330875, -0.1040454, 0.010714522]\n",
      "Iter 650, loss [-0.0912324, -0.10464389, 0.013411495]\n",
      "Iter 651, loss [-0.09090729, -0.10615016, 0.015242867]\n",
      "Iter 652, loss [-0.08419123, -0.096683465, 0.012492235]\n",
      "Iter 653, loss [-0.091696724, -0.10641582, 0.014719101]\n",
      "Iter 654, loss [-0.064788155, -0.07808923, 0.0133010745]\n",
      "Iter 655, loss [-0.09196283, -0.10700602, 0.01504319]\n",
      "Iter 656, loss [-0.2997519, -0.301936, 0.0021840944]\n",
      "Iter 657, loss [-0.0771874, -0.0920006, 0.014813205]\n",
      "Iter 658, loss [-0.06466412, -0.077317275, 0.01265316]\n",
      "Iter 659, loss [-0.06446901, -0.07677519, 0.012306182]\n",
      "Iter 660, loss [-0.085414015, -0.09849324, 0.013079226]\n",
      "Iter 661, loss [-0.30691782, -0.3087976, 0.001879773]\n",
      "Iter 662, loss [-0.073709235, -0.09245064, 0.018741403]\n",
      "Iter 663, loss [-0.076422945, -0.09087072, 0.014447778]\n",
      "Iter 664, loss [-0.08409066, -0.09538116, 0.011290507]\n",
      "Iter 665, loss [-0.08487579, -0.09599872, 0.011122929]\n",
      "Iter 666, loss [-0.08514292, -0.09725421, 0.012111293]\n",
      "Iter 667, loss [-0.08533674, -0.09652508, 0.011188345]\n",
      "Iter 668, loss [-0.09243289, -0.1053951, 0.012962213]\n",
      "Iter 669, loss [-0.30878207, -0.31021494, 0.0014328522]\n",
      "Iter 670, loss [-0.0887369, -0.10270968, 0.013972783]\n",
      "Iter 671, loss [-0.08889348, -0.10111423, 0.012220748]\n",
      "Iter 672, loss [-0.09062933, -0.104695156, 0.014065824]\n",
      "Iter 673, loss [-0.077022366, -0.091500245, 0.014477881]\n",
      "Iter 674, loss [-0.07803025, -0.09439976, 0.016369509]\n",
      "Iter 675, loss [-0.09183775, -0.107129164, 0.015291419]\n",
      "Iter 676, loss [-0.06528154, -0.07836878, 0.013087246]\n",
      "Iter 677, loss [-0.093051866, -0.10825815, 0.015206283]\n",
      "Iter 678, loss [-0.06589258, -0.07907374, 0.013181161]\n",
      "Iter 679, loss [-0.09454836, -0.10928008, 0.014731724]\n",
      "Iter 680, loss [-0.09095648, -0.104527526, 0.013571043]\n",
      "Iter 681, loss [-0.07834235, -0.09285263, 0.014510283]\n",
      "Iter 682, loss [-0.09040384, -0.1066305, 0.016226657]\n",
      "Iter 683, loss [-0.09181353, -0.105335444, 0.013521917]\n",
      "Iter 684, loss [-0.08695594, -0.09935822, 0.012402279]\n",
      "Iter 685, loss [-0.07663916, -0.09701331, 0.020374145]\n",
      "Iter 686, loss [-0.08764333, -0.09966718, 0.012023847]\n",
      "Iter 687, loss [-0.088454485, -0.10143538, 0.012980892]\n",
      "Iter 688, loss [-0.0921944, -0.1052935, 0.013099095]\n",
      "Iter 689, loss [-0.30352432, -0.30522394, 0.0016996334]\n",
      "Iter 690, loss [-0.06652373, -0.079081446, 0.012557714]\n",
      "Iter 691, loss [-0.094479874, -0.107639596, 0.013159718]\n",
      "Iter 692, loss [-0.0747153, -0.09392307, 0.019207764]\n",
      "Iter 693, loss [-0.30445078, -0.30582267, 0.0013718938]\n",
      "Iter 694, loss [-0.095824964, -0.10642073, 0.0105957715]\n",
      "Iter 695, loss [-0.063669585, -0.07473488, 0.011065299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 696, loss [-0.08913889, -0.102626964, 0.013488077]\n",
      "Iter 697, loss [-0.08839189, -0.10240046, 0.014008565]\n",
      "Iter 698, loss [-0.080583304, -0.09207467, 0.011491366]\n",
      "Iter 699, loss [-0.07562286, -0.09401487, 0.018392015]\n",
      "Iter 700, loss [-0.09609714, -0.10688532, 0.010788178]\n",
      "Iter 701, loss [-0.0865033, -0.09861178, 0.012108481]\n",
      "Iter 702, loss [-0.077743314, -0.09308074, 0.0153374225]\n",
      "Iter 703, loss [-0.086566046, -0.09995664, 0.013390595]\n",
      "Iter 704, loss [-0.09091539, -0.10389634, 0.012980951]\n",
      "Iter 705, loss [-0.092267044, -0.105260886, 0.012993845]\n",
      "Iter 706, loss [-0.085793756, -0.10086415, 0.0150703965]\n",
      "Iter 707, loss [-0.06436834, -0.076425925, 0.012057586]\n",
      "Iter 708, loss [-0.092536554, -0.10701578, 0.014479224]\n",
      "Iter 709, loss [-0.090155296, -0.10484928, 0.014693982]\n",
      "Iter 710, loss [-0.0691376, -0.082982756, 0.013845153]\n",
      "Iter 711, loss [-0.09284119, -0.10364457, 0.010803378]\n",
      "Iter 712, loss [-0.06624688, -0.078160755, 0.011913873]\n",
      "Iter 713, loss [-0.09773873, -0.10887713, 0.011138402]\n",
      "Iter 714, loss [-0.09266651, -0.1056103, 0.012943793]\n",
      "Iter 715, loss [-0.06626632, -0.079100594, 0.012834275]\n",
      "Iter 716, loss [-0.093536496, -0.10816794, 0.014631443]\n",
      "Iter 717, loss [-0.30107853, -0.30257872, 0.0015002012]\n",
      "Iter 718, loss [-0.08010699, -0.094194025, 0.01408704]\n",
      "Iter 719, loss [-0.09684177, -0.110174105, 0.013332341]\n",
      "Iter 720, loss [-0.30359468, -0.30515093, 0.0015562535]\n",
      "Iter 721, loss [-0.09130852, -0.10547515, 0.014166634]\n",
      "Iter 722, loss [-0.09149401, -0.104712695, 0.013218686]\n",
      "Iter 723, loss [-0.09081487, -0.101610556, 0.010795686]\n",
      "Iter 724, loss [-0.0937349, -0.10617578, 0.012440885]\n",
      "Iter 725, loss [-0.07973346, -0.09327996, 0.013546499]\n",
      "Iter 726, loss [-0.08715905, -0.09845625, 0.011297196]\n",
      "Iter 727, loss [-0.06751264, -0.07988765, 0.012375014]\n",
      "Iter 728, loss [-0.06808837, -0.08148088, 0.013392512]\n",
      "Iter 729, loss [-0.098919846, -0.111721024, 0.012801176]\n",
      "Iter 730, loss [-0.080328844, -0.09609359, 0.015764741]\n",
      "Iter 731, loss [-0.09362529, -0.11024421, 0.016618919]\n",
      "Iter 732, loss [-0.08855033, -0.10314712, 0.01459679]\n",
      "Iter 733, loss [-0.10026581, -0.11592246, 0.015656648]\n",
      "Iter 734, loss [-0.094671845, -0.11268179, 0.018009942]\n",
      "Iter 735, loss [-0.08869494, -0.103675544, 0.014980605]\n",
      "Iter 736, loss [-0.09399957, -0.11667237, 0.022672795]\n",
      "Iter 737, loss [-0.0785269, -0.10404063, 0.025513735]\n",
      "Iter 738, loss [-0.09692879, -0.11703902, 0.020110227]\n",
      "Iter 739, loss [-0.3069658, -0.30871925, 0.0017534511]\n",
      "Iter 740, loss [-0.09732964, -0.11439344, 0.017063804]\n",
      "Iter 741, loss [-0.06736988, -0.080029085, 0.01265921]\n",
      "Iter 742, loss [-0.0795892, -0.091740996, 0.012151793]\n",
      "Iter 743, loss [-0.079336405, -0.092597045, 0.013260638]\n",
      "Iter 744, loss [-0.08824299, -0.098653786, 0.010410793]\n",
      "Iter 745, loss [-0.09465626, -0.10971717, 0.015060907]\n",
      "Iter 746, loss [-0.095220394, -0.110297956, 0.015077559]\n",
      "Iter 747, loss [-0.09084768, -0.10416337, 0.013315691]\n",
      "Iter 748, loss [-0.068706505, -0.08162135, 0.012914842]\n",
      "Iter 749, loss [-0.06926744, -0.08284526, 0.013577828]\n",
      "Iter 750, loss [-0.090237044, -0.10481508, 0.014578037]\n",
      "Iter 751, loss [-0.09222273, -0.10836563, 0.016142903]\n",
      "Iter 752, loss [-0.101478055, -0.11589569, 0.014417633]\n",
      "Iter 753, loss [-0.0984541, -0.11702074, 0.018566636]\n",
      "Iter 754, loss [-0.08101929, -0.098520696, 0.017501405]\n",
      "Iter 755, loss [-0.10154747, -0.11694619, 0.015398717]\n",
      "Iter 756, loss [-0.096873224, -0.11384003, 0.016966805]\n",
      "Iter 757, loss [-0.09243988, -0.10813315, 0.01569327]\n",
      "Iter 758, loss [-0.08008487, -0.10556876, 0.025483891]\n",
      "Iter 759, loss [-0.30199444, -0.3038779, 0.0018834397]\n",
      "Iter 760, loss [-0.09326265, -0.110360794, 0.017098146]\n",
      "Iter 761, loss [-0.09403746, -0.10921313, 0.0151756685]\n",
      "Iter 762, loss [-0.0840192, -0.10060186, 0.016582664]\n",
      "Iter 763, loss [-0.10098897, -0.11360935, 0.012620378]\n",
      "Iter 764, loss [-0.09932569, -0.11379965, 0.014473956]\n",
      "Iter 765, loss [-0.09400294, -0.108382694, 0.014379753]\n",
      "Iter 766, loss [-0.10103596, -0.116349645, 0.015313683]\n",
      "Iter 767, loss [-0.09454286, -0.11030407, 0.01576121]\n",
      "Iter 768, loss [-0.085261755, -0.10223376, 0.016972005]\n",
      "Iter 769, loss [-0.09551494, -0.11182369, 0.016308758]\n",
      "Iter 770, loss [-0.09610457, -0.11294912, 0.016844546]\n",
      "Iter 771, loss [-0.0968204, -0.114162065, 0.017341668]\n",
      "Iter 772, loss [-0.0831962, -0.103050634, 0.019854432]\n",
      "Iter 773, loss [-0.08101249, -0.10831936, 0.02730687]\n",
      "Iter 774, loss [-0.09330818, -0.11426021, 0.020952027]\n",
      "Iter 775, loss [-0.10092856, -0.119619526, 0.018690966]\n",
      "Iter 776, loss [-0.09681988, -0.11602607, 0.0192062]\n",
      "Iter 777, loss [-0.08359318, -0.10734948, 0.023756294]\n",
      "Iter 778, loss [-0.09093135, -0.10564591, 0.014714561]\n",
      "Iter 779, loss [-0.08659841, -0.10356551, 0.016967092]\n",
      "Iter 780, loss [-0.09834437, -0.11397599, 0.015631616]\n",
      "Iter 781, loss [-0.103408895, -0.11611428, 0.012705384]\n",
      "Iter 782, loss [-0.30499765, -0.30678138, 0.0017837398]\n",
      "Iter 783, loss [-0.097877644, -0.111369684, 0.0134920385]\n",
      "Iter 784, loss [-0.10292451, -0.11519634, 0.012271828]\n",
      "Iter 785, loss [-0.095554784, -0.11031295, 0.01475817]\n",
      "Iter 786, loss [-0.09843847, -0.11212263, 0.013684161]\n",
      "Iter 787, loss [-0.07143608, -0.08530691, 0.013870838]\n",
      "Iter 788, loss [-0.08640447, -0.103274666, 0.016870191]\n",
      "Iter 789, loss [-0.10245634, -0.11952047, 0.01706413]\n",
      "Iter 790, loss [-0.30462313, -0.30648047, 0.0018573538]\n",
      "Iter 791, loss [-0.08441551, -0.10813799, 0.023722475]\n",
      "Iter 792, loss [-0.07301806, -0.08870485, 0.015686788]\n",
      "Iter 793, loss [-0.08482375, -0.10865795, 0.023834197]\n",
      "Iter 794, loss [-0.096377, -0.111171715, 0.014794714]\n",
      "Iter 795, loss [-0.10541372, -0.12212166, 0.016707938]\n",
      "Iter 796, loss [-0.10234289, -0.12104114, 0.018698249]\n",
      "Iter 797, loss [-0.09677739, -0.11189588, 0.015118495]\n",
      "Iter 798, loss [-0.097255714, -0.11250897, 0.015253253]\n",
      "Iter 799, loss [-0.085096136, -0.10995506, 0.024858922]\n",
      "Iter 800, loss [-0.1024528, -0.12226308, 0.019810282]\n",
      "Iter 801, loss [-0.09810476, -0.11376459, 0.015659835]\n",
      "Iter 802, loss [-0.10137202, -0.118419096, 0.017047081]\n",
      "Iter 803, loss [-0.08919124, -0.10930208, 0.020110838]\n",
      "Iter 804, loss [-0.100691244, -0.121751785, 0.021060538]\n",
      "Iter 805, loss [-0.10402097, -0.12441246, 0.02039149]\n",
      "Iter 806, loss [-0.3074162, -0.30923095, 0.0018147412]\n",
      "Iter 807, loss [-0.08979638, -0.10999757, 0.020201188]\n",
      "Iter 808, loss [-0.089906335, -0.10963784, 0.019731505]\n",
      "Iter 809, loss [-0.30961803, -0.31130216, 0.0016841193]\n",
      "Iter 810, loss [-0.10480688, -0.122909464, 0.018102584]\n",
      "Iter 811, loss [-0.10664293, -0.12316376, 0.01652083]\n",
      "Iter 812, loss [-0.08914782, -0.10557035, 0.01642253]\n",
      "Iter 813, loss [-0.107141525, -0.12355252, 0.016411]\n",
      "Iter 814, loss [-0.10194806, -0.11700645, 0.015058389]\n",
      "Iter 815, loss [-0.10232082, -0.11773844, 0.015417619]\n",
      "Iter 816, loss [-0.10509656, -0.12316031, 0.01806375]\n",
      "Iter 817, loss [-0.103462115, -0.12001332, 0.0165512]\n",
      "Iter 818, loss [-0.10199288, -0.12200886, 0.020015977]\n",
      "Iter 819, loss [-0.089372315, -0.10793492, 0.018562607]\n",
      "Iter 820, loss [-0.10230744, -0.12296574, 0.020658297]\n",
      "Iter 821, loss [-0.308527, -0.31032228, 0.0017952942]\n",
      "Iter 822, loss [-0.096786976, -0.112905875, 0.016118895]\n",
      "Iter 823, loss [-0.10850094, -0.12600972, 0.017508777]\n",
      "Iter 824, loss [-0.1025196, -0.11999861, 0.017479006]\n",
      "Iter 825, loss [-0.10011624, -0.11641664, 0.016300406]\n",
      "Iter 826, loss [-0.10503429, -0.1222566, 0.017222304]\n",
      "Iter 827, loss [-0.08700688, -0.11143941, 0.024432521]\n",
      "Iter 828, loss [-0.106850095, -0.12145958, 0.014609489]\n",
      "Iter 829, loss [-0.104129225, -0.12355156, 0.019422337]\n",
      "Iter 830, loss [-0.3110248, -0.31247532, 0.0014504952]\n",
      "Iter 831, loss [-0.08772828, -0.11431409, 0.026585808]\n",
      "Iter 832, loss [-0.3119011, -0.3133319, 0.0014308015]\n",
      "Iter 833, loss [-0.107252434, -0.12644139, 0.019188954]\n",
      "Iter 834, loss [-0.0890511, -0.11404969, 0.024998587]\n",
      "Iter 835, loss [-0.089377984, -0.113758445, 0.024380457]\n",
      "Iter 836, loss [-0.09218107, -0.11045028, 0.018269215]\n",
      "Iter 837, loss [-0.09121472, -0.10767998, 0.016465265]\n",
      "Iter 838, loss [-0.09238599, -0.11054733, 0.018161338]\n",
      "Iter 839, loss [-0.09011178, -0.11424085, 0.024129068]\n",
      "Iter 840, loss [-0.10712262, -0.12567656, 0.018553937]\n",
      "Iter 841, loss [-0.090703785, -0.11571374, 0.02500995]\n",
      "Iter 842, loss [-0.091093615, -0.11643265, 0.025339037]\n",
      "Iter 843, loss [-0.09981938, -0.11548773, 0.015668355]\n",
      "Iter 844, loss [-0.11070677, -0.1282143, 0.01750753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 845, loss [-0.092446074, -0.11022208, 0.017776003]\n",
      "Iter 846, loss [-0.10542384, -0.12684393, 0.02142009]\n",
      "Iter 847, loss [-0.10601714, -0.12708898, 0.021071833]\n",
      "Iter 848, loss [-0.09227741, -0.11790379, 0.025626387]\n",
      "Iter 849, loss [-0.092414126, -0.11751995, 0.02510583]\n",
      "Iter 850, loss [-0.07423914, -0.090557806, 0.016318662]\n",
      "Iter 851, loss [-0.10593103, -0.12199457, 0.016063537]\n",
      "Iter 852, loss [-0.10669678, -0.123233326, 0.016536549]\n",
      "Iter 853, loss [-0.11146757, -0.1272421, 0.015774531]\n",
      "Iter 854, loss [-0.07717404, -0.0956734, 0.018499356]\n",
      "Iter 855, loss [-0.10294616, -0.12229294, 0.01934678]\n",
      "Iter 856, loss [-0.30278268, -0.3049746, 0.002191913]\n",
      "Iter 857, loss [-0.11274003, -0.13269787, 0.019957831]\n",
      "Iter 858, loss [-0.09605554, -0.11754025, 0.021484714]\n",
      "Iter 859, loss [-0.10210642, -0.11916488, 0.017058456]\n",
      "Iter 860, loss [-0.07716179, -0.095093444, 0.017931659]\n",
      "Iter 861, loss [-0.30733287, -0.30880192, 0.0014690583]\n",
      "Iter 862, loss [-0.109271, -0.1295663, 0.0202953]\n",
      "Iter 863, loss [-0.10961367, -0.12473127, 0.015117603]\n",
      "Iter 864, loss [-0.09083606, -0.10992943, 0.019093363]\n",
      "Iter 865, loss [-0.10092597, -0.11653367, 0.015607703]\n",
      "Iter 866, loss [-0.105360486, -0.123161234, 0.017800748]\n",
      "Iter 867, loss [-0.10160148, -0.11998774, 0.01838626]\n",
      "Iter 868, loss [-0.07521061, -0.09311015, 0.017899543]\n",
      "Iter 869, loss [-0.10269159, -0.119107924, 0.016416337]\n",
      "Iter 870, loss [-0.105456844, -0.12118904, 0.015732195]\n",
      "Iter 871, loss [-0.07742698, -0.09494442, 0.01751744]\n",
      "Iter 872, loss [-0.09170504, -0.10963908, 0.01793404]\n",
      "Iter 873, loss [-0.11092448, -0.12905915, 0.018134668]\n",
      "Iter 874, loss [-0.07919106, -0.09851165, 0.019320596]\n",
      "Iter 875, loss [-0.10514363, -0.13034192, 0.025198286]\n",
      "Iter 876, loss [-0.091110826, -0.122009315, 0.030898493]\n",
      "Iter 877, loss [-0.10625593, -0.13006021, 0.023804277]\n",
      "Iter 878, loss [-0.09275628, -0.121088825, 0.028332546]\n",
      "Iter 879, loss [-0.11222257, -0.12743881, 0.015216246]\n",
      "Iter 880, loss [-0.107418664, -0.12371793, 0.016299265]\n",
      "Iter 881, loss [-0.3084374, -0.3101845, 0.0017471046]\n",
      "Iter 882, loss [-0.11498152, -0.13260138, 0.017619865]\n",
      "Iter 883, loss [-0.113231435, -0.12817043, 0.014938998]\n",
      "Iter 884, loss [-0.106145315, -0.1262359, 0.020090587]\n",
      "Iter 885, loss [-0.095300876, -0.12199073, 0.026689857]\n",
      "Iter 886, loss [-0.09540498, -0.12305823, 0.027653247]\n",
      "Iter 887, loss [-0.10959587, -0.12833063, 0.018734759]\n",
      "Iter 888, loss [-0.10191264, -0.12068837, 0.018775731]\n",
      "Iter 889, loss [-0.11646028, -0.1369633, 0.02050301]\n",
      "Iter 890, loss [-0.106074944, -0.123871356, 0.017796414]\n",
      "Iter 891, loss [-0.09750226, -0.1192877, 0.021785438]\n",
      "Iter 892, loss [-0.11535548, -0.1332461, 0.017890621]\n",
      "Iter 893, loss [-0.11621494, -0.13593104, 0.019716108]\n",
      "Iter 894, loss [-0.09807105, -0.11815274, 0.020081682]\n",
      "Iter 895, loss [-0.11005363, -0.1293368, 0.019283172]\n",
      "Iter 896, loss [-0.10959857, -0.12948261, 0.01988404]\n",
      "Iter 897, loss [-0.1115344, -0.13227412, 0.020739723]\n",
      "Iter 898, loss [-0.11879768, -0.1402044, 0.021406718]\n",
      "Iter 899, loss [-0.11189349, -0.13227117, 0.020377683]\n",
      "Iter 900, loss [-0.11294265, -0.13537279, 0.022430137]\n",
      "Iter 901, loss [-0.09650898, -0.13001168, 0.033502694]\n",
      "Iter 902, loss [-0.30208266, -0.3048844, 0.0028017326]\n",
      "Iter 903, loss [-0.30534035, -0.30776784, 0.002427503]\n",
      "Iter 904, loss [-0.11189451, -0.13078363, 0.018889122]\n",
      "Iter 905, loss [-0.110048905, -0.13071336, 0.020664452]\n",
      "Iter 906, loss [-0.09948817, -0.119633265, 0.0201451]\n",
      "Iter 907, loss [-0.111290745, -0.12887542, 0.017584674]\n",
      "Iter 908, loss [-0.1137998, -0.1284461, 0.014646298]\n",
      "Iter 909, loss [-0.112286635, -0.131068, 0.01878137]\n",
      "Iter 910, loss [-0.11043819, -0.13149747, 0.021059282]\n",
      "Iter 911, loss [-0.31146163, -0.3131699, 0.0017082698]\n",
      "Iter 912, loss [-0.10837567, -0.12835872, 0.019983053]\n",
      "Iter 913, loss [-0.11763813, -0.13511443, 0.017476296]\n",
      "Iter 914, loss [-0.099025786, -0.129663, 0.030637216]\n",
      "Iter 915, loss [-0.11271371, -0.13718669, 0.024472978]\n",
      "Iter 916, loss [-0.099174455, -0.12973456, 0.03056011]\n",
      "Iter 917, loss [-0.1154165, -0.13912724, 0.023710746]\n",
      "Iter 918, loss [-0.081578776, -0.10137669, 0.019797912]\n",
      "Iter 919, loss [-0.109685466, -0.12836497, 0.018679501]\n",
      "Iter 920, loss [-0.11452421, -0.13563897, 0.02111476]\n",
      "Iter 921, loss [-0.30531228, -0.30748865, 0.0021763854]\n",
      "Iter 922, loss [-0.08296979, -0.104500145, 0.02153035]\n",
      "Iter 923, loss [-0.12099013, -0.14265999, 0.021669863]\n",
      "Iter 924, loss [-0.10284141, -0.12675971, 0.0239183]\n",
      "Iter 925, loss [-0.112634294, -0.1381172, 0.025482899]\n",
      "Iter 926, loss [-0.118895255, -0.13663946, 0.017744206]\n",
      "Iter 927, loss [-0.08292706, -0.10251069, 0.019583624]\n",
      "Iter 928, loss [-0.1150932, -0.13866392, 0.023570713]\n",
      "Iter 929, loss [-0.31175178, -0.3134479, 0.0016961149]\n",
      "Iter 930, loss [-0.12269481, -0.14337574, 0.020680925]\n",
      "Iter 931, loss [-0.11149449, -0.13023382, 0.018739333]\n",
      "Iter 932, loss [-0.11967372, -0.13640936, 0.016735634]\n",
      "Iter 933, loss [-0.11417045, -0.13737336, 0.023202911]\n",
      "Iter 934, loss [-0.102812015, -0.12523715, 0.022425136]\n",
      "Iter 935, loss [-0.31110105, -0.31283066, 0.0017296085]\n",
      "Iter 936, loss [-0.1022066, -0.12350019, 0.021293588]\n",
      "Iter 937, loss [-0.10277615, -0.124462694, 0.021686547]\n",
      "Iter 938, loss [-0.102859795, -0.12488372, 0.02202392]\n",
      "Iter 939, loss [-0.102869004, -0.1250044, 0.022135396]\n",
      "Iter 940, loss [-0.11394392, -0.13349694, 0.019553017]\n",
      "Iter 941, loss [-0.11540036, -0.1355845, 0.02018414]\n",
      "Iter 942, loss [-0.102206655, -0.13482682, 0.03262017]\n",
      "Iter 943, loss [-0.08389355, -0.10612288, 0.022229327]\n",
      "Iter 944, loss [-0.116985776, -0.13919687, 0.022211097]\n",
      "Iter 945, loss [-0.10243594, -0.12572916, 0.02329322]\n",
      "Iter 946, loss [-0.08552544, -0.10771225, 0.022186806]\n",
      "Iter 947, loss [-0.11333924, -0.1337878, 0.020448558]\n",
      "Iter 948, loss [-0.30356222, -0.30554166, 0.0019794428]\n",
      "Iter 949, loss [-0.11055128, -0.13826302, 0.027711738]\n",
      "Iter 950, loss [-0.105163686, -0.12936859, 0.0242049]\n",
      "Iter 951, loss [-0.10150726, -0.13124631, 0.029739052]\n",
      "Iter 952, loss [-0.115094975, -0.13382867, 0.018733697]\n",
      "Iter 953, loss [-0.10363634, -0.12448075, 0.020844411]\n",
      "Iter 954, loss [-0.11005741, -0.1262058, 0.016148387]\n",
      "Iter 955, loss [-0.102330565, -0.12934513, 0.027014567]\n",
      "Iter 956, loss [-0.10996498, -0.1304574, 0.020492416]\n",
      "Iter 957, loss [-0.11336668, -0.13563237, 0.022265686]\n",
      "Iter 958, loss [-0.097776, -0.12541573, 0.027639724]\n",
      "Iter 959, loss [-0.09104474, -0.11502483, 0.023980087]\n",
      "Iter 960, loss [-0.10510231, -0.13023122, 0.025128907]\n",
      "Iter 961, loss [-0.117472515, -0.13760947, 0.020136952]\n",
      "Iter 962, loss [-0.29965302, -0.30234438, 0.0026913635]\n",
      "Iter 963, loss [-0.10005154, -0.11877516, 0.018723626]\n",
      "Iter 964, loss [-0.11896974, -0.13595712, 0.016987387]\n",
      "Iter 965, loss [-0.110922456, -0.13502802, 0.024105564]\n",
      "Iter 966, loss [-0.11534903, -0.13635555, 0.021006515]\n",
      "Iter 967, loss [-0.102032065, -0.12555273, 0.023520663]\n",
      "Iter 968, loss [-0.0809284, -0.096503936, 0.015575538]\n",
      "Iter 969, loss [-0.3077115, -0.3089779, 0.0012664024]\n",
      "Iter 970, loss [-0.101110496, -0.1198516, 0.018741101]\n",
      "Iter 971, loss [-0.12046875, -0.13764334, 0.017174587]\n",
      "Iter 972, loss [-0.3118467, -0.31319433, 0.0013476319]\n",
      "Iter 973, loss [-0.104103245, -0.12486432, 0.02076107]\n",
      "Iter 974, loss [-0.3129318, -0.31433782, 0.001406012]\n",
      "Iter 975, loss [-0.313813, -0.31513324, 0.0013202446]\n",
      "Iter 976, loss [-0.10787719, -0.13326304, 0.02538585]\n",
      "Iter 977, loss [-0.08312575, -0.10177853, 0.018652784]\n",
      "Iter 978, loss [-0.11717166, -0.13127294, 0.014101278]\n",
      "Iter 979, loss [-0.122474596, -0.14074671, 0.018272113]\n",
      "Iter 980, loss [-0.11640097, -0.13615209, 0.019751113]\n",
      "Iter 981, loss [-0.084872514, -0.104972705, 0.020100188]\n",
      "Iter 982, loss [-0.12228198, -0.13984217, 0.017560188]\n",
      "Iter 983, loss [-0.12425205, -0.14404993, 0.01979788]\n",
      "Iter 984, loss [-0.087136276, -0.11279524, 0.025658965]\n",
      "Iter 985, loss [-0.087665275, -0.11492256, 0.027257286]\n",
      "Iter 986, loss [-0.12461766, -0.1502318, 0.025614133]\n",
      "Iter 987, loss [-0.10341676, -0.13341683, 0.030000068]\n",
      "Iter 988, loss [-0.11292244, -0.13911387, 0.026191434]\n",
      "Iter 989, loss [-0.10574755, -0.13451906, 0.028771505]\n",
      "Iter 990, loss [-0.11804111, -0.14361326, 0.025572151]\n",
      "Iter 991, loss [-0.08734502, -0.11240105, 0.025056034]\n",
      "Iter 992, loss [-0.12047499, -0.14320876, 0.022733768]\n",
      "Iter 993, loss [-0.11975928, -0.1485871, 0.028827816]\n",
      "Iter 994, loss [-0.118062004, -0.14646132, 0.028399322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 995, loss [-0.1066549, -0.12899417, 0.022339273]\n",
      "Iter 996, loss [-0.12090537, -0.14825946, 0.02735409]\n",
      "Iter 997, loss [-0.12766084, -0.14918517, 0.02152433]\n",
      "Iter 998, loss [-0.106088124, -0.13853201, 0.03244389]\n",
      "Iter 999, loss [-0.11594364, -0.13414566, 0.01820202]\n",
      "Iter 1000, loss [-0.305537, -0.30831754, 0.002780564]\n",
      "Iter 1001, loss [-0.107177936, -0.12786978, 0.02069185]\n",
      "Iter 1002, loss [-0.10689409, -0.13697, 0.030075911]\n",
      "Iter 1003, loss [-0.12011302, -0.13897865, 0.01886562]\n",
      "Iter 1004, loss [-0.10755235, -0.13669397, 0.029141616]\n",
      "Iter 1005, loss [-0.12838604, -0.1482994, 0.019913359]\n",
      "Iter 1006, loss [-0.10828038, -0.1288271, 0.020546714]\n",
      "Iter 1007, loss [-0.10888438, -0.13015974, 0.021275356]\n",
      "Iter 1008, loss [-0.12138948, -0.14478514, 0.023395656]\n",
      "Iter 1009, loss [-0.12996621, -0.15273091, 0.022764694]\n",
      "Iter 1010, loss [-0.108541265, -0.14232719, 0.033785928]\n",
      "Iter 1011, loss [-0.122372895, -0.14759265, 0.025219752]\n",
      "Iter 1012, loss [-0.12806284, -0.14915986, 0.021097017]\n",
      "Iter 1013, loss [-0.12279853, -0.14709319, 0.02429466]\n",
      "Iter 1014, loss [-0.11858681, -0.14152624, 0.022939432]\n",
      "Iter 1015, loss [-0.30488166, -0.3071223, 0.0022406233]\n",
      "Iter 1016, loss [-0.121058956, -0.14526367, 0.02420472]\n",
      "Iter 1017, loss [-0.121697694, -0.14635482, 0.024657127]\n",
      "Iter 1018, loss [-0.31075826, -0.31251562, 0.0017573403]\n",
      "Iter 1019, loss [-0.31247774, -0.31400478, 0.0015270286]\n",
      "Iter 1020, loss [-0.1306082, -0.15384099, 0.02323278]\n",
      "Iter 1021, loss [-0.109055534, -0.1318627, 0.022807162]\n",
      "Iter 1022, loss [-0.11846816, -0.1394124, 0.020944243]\n",
      "Iter 1023, loss [-0.11050926, -0.13418768, 0.023678418]\n",
      "Iter 1024, loss [-0.11071196, -0.1343658, 0.023653835]\n",
      "Iter 1025, loss [-0.118543245, -0.1377395, 0.019196251]\n",
      "Iter 1026, loss [-0.1263684, -0.14508742, 0.01871902]\n",
      "Iter 1027, loss [-0.13072501, -0.15418617, 0.023461163]\n",
      "Iter 1028, loss [-0.12182704, -0.15109096, 0.02926392]\n",
      "Iter 1029, loss [-0.11999562, -0.14184368, 0.021848058]\n",
      "Iter 1030, loss [-0.109145686, -0.14702205, 0.037876364]\n",
      "Iter 1031, loss [-0.31207907, -0.31384033, 0.0017612465]\n",
      "Iter 1032, loss [-0.112497166, -0.13999957, 0.027502399]\n",
      "Iter 1033, loss [-0.11254477, -0.1394148, 0.026870035]\n",
      "Iter 1034, loss [-0.122429706, -0.14524232, 0.02281261]\n",
      "Iter 1035, loss [-0.11102539, -0.13466002, 0.023634631]\n",
      "Iter 1036, loss [-0.1112894, -0.13481458, 0.023525182]\n",
      "Iter 1037, loss [-0.12476219, -0.14685318, 0.022090986]\n",
      "Iter 1038, loss [-0.11347536, -0.14004493, 0.026569566]\n",
      "Iter 1039, loss [-0.12098639, -0.1477004, 0.02671401]\n",
      "Iter 1040, loss [-0.12486022, -0.15212518, 0.02726496]\n",
      "Iter 1041, loss [-0.11129281, -0.1477103, 0.03641748]\n",
      "Iter 1042, loss [-0.1224521, -0.14904694, 0.02659484]\n",
      "Iter 1043, loss [-0.31549773, -0.31664258, 0.0011448709]\n",
      "Iter 1044, loss [-0.111170165, -0.13548607, 0.024315901]\n",
      "Iter 1045, loss [-0.12190045, -0.1424194, 0.02051895]\n",
      "Iter 1046, loss [-0.1128269, -0.14520675, 0.032379854]\n",
      "Iter 1047, loss [-0.11437103, -0.13961652, 0.025245488]\n",
      "Iter 1048, loss [-0.090388775, -0.111780114, 0.021391336]\n",
      "Iter 1049, loss [-0.1258513, -0.14829916, 0.02244786]\n",
      "Iter 1050, loss [-0.11274567, -0.14717387, 0.034428194]\n",
      "Iter 1051, loss [-0.3153391, -0.3165253, 0.0011862123]\n",
      "Iter 1052, loss [-0.12667158, -0.15254778, 0.025876194]\n",
      "Iter 1053, loss [-0.09204479, -0.11510068, 0.023055889]\n",
      "Iter 1054, loss [-0.115781836, -0.14346428, 0.027682444]\n",
      "Iter 1055, loss [-0.11234759, -0.13792466, 0.025577066]\n",
      "Iter 1056, loss [-0.1202293, -0.14278343, 0.022554135]\n",
      "Iter 1057, loss [-0.12133519, -0.1434161, 0.022080913]\n",
      "Iter 1058, loss [-0.092149034, -0.11620811, 0.02405907]\n",
      "Iter 1059, loss [-0.11303073, -0.14858206, 0.035551324]\n",
      "Iter 1060, loss [-0.12210438, -0.14349158, 0.021387205]\n",
      "Iter 1061, loss [-0.13073987, -0.15116309, 0.020423215]\n",
      "Iter 1062, loss [-0.12624083, -0.15256716, 0.026326334]\n",
      "Iter 1063, loss [-0.11381362, -0.1498039, 0.03599029]\n",
      "Iter 1064, loss [-0.113721855, -0.13874306, 0.025021201]\n",
      "Iter 1065, loss [-0.093173355, -0.11739238, 0.024219032]\n",
      "Iter 1066, loss [-0.13244046, -0.15459524, 0.022154778]\n",
      "Iter 1067, loss [-0.12478475, -0.15031406, 0.025529308]\n",
      "Iter 1068, loss [-0.123974696, -0.14636041, 0.022385713]\n",
      "Iter 1069, loss [-0.12792721, -0.15258451, 0.0246573]\n",
      "Iter 1070, loss [-0.12550142, -0.15876919, 0.033267774]\n",
      "Iter 1071, loss [-0.12731971, -0.15855013, 0.03123042]\n",
      "Iter 1072, loss [-0.12548758, -0.14984877, 0.024361199]\n",
      "Iter 1073, loss [-0.13229355, -0.15366906, 0.021375503]\n",
      "Iter 1074, loss [-0.12748621, -0.15239677, 0.02491055]\n",
      "Iter 1075, loss [-0.12448147, -0.14536001, 0.020878535]\n",
      "Iter 1076, loss [-0.12825267, -0.1535026, 0.025249925]\n",
      "Iter 1077, loss [-0.09349479, -0.117726445, 0.024231661]\n",
      "Iter 1078, loss [-0.11304641, -0.1499111, 0.036864694]\n",
      "Iter 1079, loss [-0.09492971, -0.11985801, 0.024928298]\n",
      "Iter 1080, loss [-0.1287872, -0.15651318, 0.027725978]\n",
      "Iter 1081, loss [-0.12476617, -0.15199642, 0.027230248]\n",
      "Iter 1082, loss [-0.12800784, -0.15316415, 0.025156304]\n",
      "Iter 1083, loss [-0.13362226, -0.15604956, 0.022427313]\n",
      "Iter 1084, loss [-0.11515027, -0.15088522, 0.03573495]\n",
      "Iter 1085, loss [-0.11524655, -0.15023626, 0.034989715]\n",
      "Iter 1086, loss [-0.12878974, -0.15448996, 0.02570023]\n",
      "Iter 1087, loss [-0.12919389, -0.15464637, 0.025452476]\n",
      "Iter 1088, loss [-0.31164157, -0.31317514, 0.0015335754]\n",
      "Iter 1089, loss [-0.12794024, -0.1508139, 0.022873677]\n",
      "Iter 1090, loss [-0.11711168, -0.14362977, 0.026518095]\n",
      "Iter 1091, loss [-0.13625544, -0.16016501, 0.023909565]\n",
      "Iter 1092, loss [-0.1248111, -0.15086071, 0.026049614]\n",
      "Iter 1093, loss [-0.11717431, -0.14399785, 0.026823532]\n",
      "Iter 1094, loss [-0.11748799, -0.14426582, 0.026777824]\n",
      "Iter 1095, loss [-0.11807835, -0.14492774, 0.026849385]\n",
      "Iter 1096, loss [-0.12657866, -0.15180895, 0.025230285]\n",
      "Iter 1097, loss [-0.09436759, -0.117807, 0.023439407]\n",
      "Iter 1098, loss [-0.115177155, -0.14084299, 0.025665835]\n",
      "Iter 1099, loss [-0.31479576, -0.31599614, 0.0012003884]\n",
      "Iter 1100, loss [-0.12978229, -0.15422806, 0.024445768]\n",
      "Iter 1101, loss [-0.12674485, -0.15065207, 0.023907213]\n",
      "Iter 1102, loss [-0.13702908, -0.16296521, 0.025936123]\n",
      "Iter 1103, loss [-0.09600497, -0.122324206, 0.026319236]\n",
      "Iter 1104, loss [-0.31528944, -0.3164329, 0.0011434585]\n",
      "Iter 1105, loss [-0.12726472, -0.15254948, 0.025284756]\n",
      "Iter 1106, loss [-0.13485645, -0.15820165, 0.023345208]\n",
      "Iter 1107, loss [-0.12750715, -0.1530702, 0.025563043]\n",
      "Iter 1108, loss [-0.13534331, -0.15914497, 0.023801662]\n",
      "Iter 1109, loss [-0.116283834, -0.14380592, 0.02752209]\n",
      "Iter 1110, loss [-0.116187, -0.15533458, 0.03914758]\n",
      "Iter 1111, loss [-0.13056974, -0.15458176, 0.024012014]\n",
      "Iter 1112, loss [-0.12833987, -0.15738289, 0.029043023]\n",
      "Iter 1113, loss [-0.117775366, -0.15379708, 0.036021706]\n",
      "Iter 1114, loss [-0.124581076, -0.14569153, 0.02111045]\n",
      "Iter 1115, loss [-0.12793271, -0.15491702, 0.026984304]\n",
      "Iter 1116, loss [-0.11620085, -0.14085892, 0.024658067]\n",
      "Iter 1117, loss [-0.13182265, -0.15917484, 0.027352199]\n",
      "Iter 1118, loss [-0.11884321, -0.14641337, 0.027570162]\n",
      "Iter 1119, loss [-0.096357666, -0.120652296, 0.024294628]\n",
      "Iter 1120, loss [-0.119794935, -0.14790645, 0.028111514]\n",
      "Iter 1121, loss [-0.117714494, -0.15400515, 0.036290657]\n",
      "Iter 1122, loss [-0.31552508, -0.31656724, 0.0010421572]\n",
      "Iter 1123, loss [-0.13734257, -0.16180652, 0.024463948]\n",
      "Iter 1124, loss [-0.124696456, -0.14619899, 0.02150253]\n",
      "Iter 1125, loss [-0.31702286, -0.31782553, 0.00080267456]\n",
      "Iter 1126, loss [-0.11670544, -0.14141856, 0.02471312]\n",
      "Iter 1127, loss [-0.31775203, -0.31842086, 0.00066881155]\n",
      "Iter 1128, loss [-0.12909073, -0.15236214, 0.02327141]\n",
      "Iter 1129, loss [-0.1234603, -0.14374958, 0.02028928]\n",
      "Iter 1130, loss [-0.12827386, -0.15319747, 0.024923611]\n",
      "Iter 1131, loss [-0.13621932, -0.15908127, 0.022861939]\n",
      "Iter 1132, loss [-0.3160326, -0.31706473, 0.0010321578]\n",
      "Iter 1133, loss [-0.12062154, -0.1509081, 0.030286558]\n",
      "Iter 1134, loss [-0.12423222, -0.1479333, 0.023701083]\n",
      "Iter 1135, loss [-0.13157803, -0.1571383, 0.025560278]\n",
      "Iter 1136, loss [-0.11784451, -0.14558236, 0.027737854]\n",
      "Iter 1137, loss [-0.13855116, -0.16524324, 0.026692083]\n",
      "Iter 1138, loss [-0.12816234, -0.15650325, 0.02834091]\n",
      "Iter 1139, loss [-0.11740525, -0.14347832, 0.026073068]\n",
      "Iter 1140, loss [-0.13763884, -0.16119038, 0.023551537]\n",
      "Iter 1141, loss [-0.13027778, -0.1620505, 0.031772725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1142, loss [-0.13721602, -0.15914546, 0.02192944]\n",
      "Iter 1143, loss [-0.1257019, -0.14552848, 0.019826574]\n",
      "Iter 1144, loss [-0.11789619, -0.1417777, 0.023881502]\n",
      "Iter 1145, loss [-0.12704355, -0.1579306, 0.030887058]\n",
      "Iter 1146, loss [-0.13192788, -0.15554745, 0.023619583]\n",
      "Iter 1147, loss [-0.13945796, -0.16491215, 0.02545419]\n",
      "Iter 1148, loss [-0.13213152, -0.16100435, 0.028872833]\n",
      "Iter 1149, loss [-0.097515166, -0.123331726, 0.02581656]\n",
      "Iter 1150, loss [-0.1390765, -0.1668927, 0.027816214]\n",
      "Iter 1151, loss [-0.11865227, -0.14721125, 0.028558984]\n",
      "Iter 1152, loss [-0.11906934, -0.14729613, 0.028226793]\n",
      "Iter 1153, loss [-0.121035025, -0.15103064, 0.029995618]\n",
      "Iter 1154, loss [-0.12895024, -0.15381007, 0.024859827]\n",
      "Iter 1155, loss [-0.11896956, -0.15563263, 0.03666307]\n",
      "Iter 1156, loss [-0.12804857, -0.15080637, 0.022757804]\n",
      "Iter 1157, loss [-0.140553, -0.16614024, 0.025587242]\n",
      "Iter 1158, loss [-0.12907295, -0.15342455, 0.024351595]\n",
      "Iter 1159, loss [-0.31521165, -0.3164863, 0.0012746342]\n",
      "Iter 1160, loss [-0.13803825, -0.1610677, 0.023029452]\n",
      "Iter 1161, loss [-0.1404095, -0.16750209, 0.027092583]\n",
      "Iter 1162, loss [-0.13021666, -0.15475066, 0.024534006]\n",
      "Iter 1163, loss [-0.12903452, -0.15496935, 0.025934828]\n",
      "Iter 1164, loss [-0.1303983, -0.15317978, 0.022781476]\n",
      "Iter 1165, loss [-0.13069695, -0.15320323, 0.022506278]\n",
      "Iter 1166, loss [-0.14138313, -0.16641192, 0.025028793]\n",
      "Iter 1167, loss [-0.12019886, -0.15640199, 0.036203135]\n",
      "Iter 1168, loss [-0.13354751, -0.15910901, 0.02556149]\n",
      "Iter 1169, loss [-0.09811923, -0.12411896, 0.025999732]\n",
      "Iter 1170, loss [-0.09866454, -0.12533033, 0.026665796]\n",
      "Iter 1171, loss [-0.09917403, -0.12694895, 0.02777492]\n",
      "Iter 1172, loss [-0.12937218, -0.1561831, 0.02681092]\n",
      "Iter 1173, loss [-0.118468046, -0.14907087, 0.030602828]\n",
      "Iter 1174, loss [-0.13352622, -0.16218254, 0.028656319]\n",
      "Iter 1175, loss [-0.1289908, -0.16070008, 0.03170928]\n",
      "Iter 1176, loss [-0.12038553, -0.15872641, 0.038340878]\n",
      "Iter 1177, loss [-0.12979189, -0.15639569, 0.026603803]\n",
      "Iter 1178, loss [-0.13001147, -0.15545586, 0.025444392]\n",
      "Iter 1179, loss [-0.13061412, -0.15639058, 0.02577646]\n",
      "Iter 1180, loss [-0.096604854, -0.11898395, 0.022379093]\n",
      "Iter 1181, loss [-0.316176, -0.31716317, 0.0009871572]\n",
      "Iter 1182, loss [-0.12758952, -0.15779884, 0.030209322]\n",
      "Iter 1183, loss [-0.31724787, -0.31807065, 0.00082277576]\n",
      "Iter 1184, loss [-0.1270653, -0.14487821, 0.017812915]\n",
      "Iter 1185, loss [-0.12783468, -0.14803536, 0.020200681]\n",
      "Iter 1186, loss [-0.12904184, -0.15755548, 0.028513638]\n",
      "Iter 1187, loss [-0.13378567, -0.157233, 0.023447327]\n",
      "Iter 1188, loss [-0.12058956, -0.15663245, 0.03604289]\n",
      "Iter 1189, loss [-0.31685972, -0.31771612, 0.00085639645]\n",
      "Iter 1190, loss [-0.14157806, -0.16777968, 0.026201617]\n",
      "Iter 1191, loss [-0.100134626, -0.12602942, 0.025894787]\n",
      "Iter 1192, loss [-0.1313361, -0.15604241, 0.024706317]\n",
      "Iter 1193, loss [-0.12133181, -0.15814734, 0.03681552]\n",
      "Iter 1194, loss [-0.1421029, -0.16965821, 0.027555324]\n",
      "Iter 1195, loss [-0.12917602, -0.15249006, 0.023314051]\n",
      "Iter 1196, loss [-0.121690646, -0.15861186, 0.03692122]\n",
      "Iter 1197, loss [-0.099986136, -0.12714066, 0.02715452]\n",
      "Iter 1198, loss [-0.1228186, -0.15302731, 0.030208712]\n",
      "Iter 1199, loss [-0.1347054, -0.16166535, 0.026959948]\n",
      "Iter 1200, loss [-0.12351948, -0.153437, 0.029917523]\n",
      "Iter 1201, loss [-0.14276686, -0.16951802, 0.026751166]\n",
      "Iter 1202, loss [-0.13161951, -0.15555133, 0.023931809]\n",
      "Iter 1203, loss [-0.122637026, -0.15946175, 0.036824726]\n",
      "Iter 1204, loss [-0.132307, -0.16089408, 0.028587094]\n",
      "Iter 1205, loss [-0.31815425, -0.31880784, 0.0006535825]\n",
      "Iter 1206, loss [-0.13020767, -0.15168875, 0.021481087]\n",
      "Iter 1207, loss [-0.13298608, -0.15854627, 0.025560189]\n",
      "Iter 1208, loss [-0.12835246, -0.14810665, 0.019754186]\n",
      "Iter 1209, loss [-0.31809115, -0.31875163, 0.0006604706]\n",
      "Iter 1210, loss [-0.31820133, -0.31884181, 0.00064049603]\n",
      "Iter 1211, loss [-0.1312617, -0.15928864, 0.028026937]\n",
      "Iter 1212, loss [-0.13781455, -0.15865378, 0.020839226]\n",
      "Iter 1213, loss [-0.11936404, -0.14518659, 0.025822546]\n",
      "Iter 1214, loss [-0.10121392, -0.12810041, 0.026886493]\n",
      "Iter 1215, loss [-0.13890764, -0.16166747, 0.022759821]\n",
      "Iter 1216, loss [-0.120547846, -0.1481264, 0.027578544]\n",
      "Iter 1217, loss [-0.31712964, -0.31774104, 0.0006113993]\n",
      "Iter 1218, loss [-0.101110905, -0.12972414, 0.028613236]\n",
      "Iter 1219, loss [-0.124418125, -0.15539029, 0.030972164]\n",
      "Iter 1220, loss [-0.10144001, -0.1303421, 0.02890208]\n",
      "Iter 1221, loss [-0.31915185, -0.31967047, 0.00051861483]\n",
      "Iter 1222, loss [-0.10144033, -0.13101481, 0.029574474]\n",
      "Iter 1223, loss [-0.1320269, -0.15797257, 0.025945678]\n",
      "Iter 1224, loss [-0.12771437, -0.15466484, 0.026950484]\n",
      "Iter 1225, loss [-0.12103318, -0.15964548, 0.038612302]\n",
      "Iter 1226, loss [-0.13078588, -0.15424095, 0.023455061]\n",
      "Iter 1227, loss [-0.1270552, -0.15397577, 0.026920568]\n",
      "Iter 1228, loss [-0.13079765, -0.15517353, 0.024375878]\n",
      "Iter 1229, loss [-0.13383931, -0.1570807, 0.023241391]\n",
      "Iter 1230, loss [-0.13599679, -0.15458564, 0.01858886]\n",
      "Iter 1231, loss [-0.12992275, -0.15017731, 0.02025457]\n",
      "Iter 1232, loss [-0.12777475, -0.15498029, 0.027205542]\n",
      "Iter 1233, loss [-0.12292991, -0.15961978, 0.03668987]\n",
      "Iter 1234, loss [-0.31121123, -0.3128627, 0.0016514694]\n",
      "Iter 1235, loss [-0.11955138, -0.15701294, 0.037461556]\n",
      "Iter 1236, loss [-0.12857959, -0.15219787, 0.02361828]\n",
      "Iter 1237, loss [-0.124307185, -0.14490665, 0.020599471]\n",
      "Iter 1238, loss [-0.11798279, -0.14008193, 0.022099134]\n",
      "Iter 1239, loss [-0.13035803, -0.14945903, 0.019101007]\n",
      "Iter 1240, loss [-0.116108, -0.14204645, 0.025938448]\n",
      "Iter 1241, loss [-0.124583334, -0.14417268, 0.01958935]\n",
      "Iter 1242, loss [-0.116366446, -0.14160497, 0.025238529]\n",
      "Iter 1243, loss [-0.12106615, -0.15093997, 0.029873816]\n",
      "Iter 1244, loss [-0.13899934, -0.16255745, 0.023558116]\n",
      "Iter 1245, loss [-0.118124254, -0.14975673, 0.031632476]\n",
      "Iter 1246, loss [-0.11817351, -0.15064323, 0.03246972]\n",
      "Iter 1247, loss [-0.123381935, -0.16006003, 0.0366781]\n",
      "Iter 1248, loss [-0.13084172, -0.16387121, 0.033029497]\n",
      "Iter 1249, loss [-0.13286856, -0.16141114, 0.028542582]\n",
      "Iter 1250, loss [-0.120648935, -0.14901577, 0.028366838]\n",
      "Iter 1251, loss [-0.09433448, -0.115880795, 0.021546315]\n",
      "Iter 1252, loss [-0.1271214, -0.14788018, 0.020758772]\n",
      "Iter 1253, loss [-0.12211134, -0.15598539, 0.033874042]\n",
      "Iter 1254, loss [-0.1314444, -0.15523505, 0.023790656]\n",
      "Iter 1255, loss [-0.14062592, -0.16322833, 0.022602402]\n",
      "Iter 1256, loss [-0.14266048, -0.16847578, 0.025815291]\n",
      "Iter 1257, loss [-0.12230064, -0.15912437, 0.03682373]\n",
      "Iter 1258, loss [-0.1358378, -0.16109277, 0.025254987]\n",
      "Iter 1259, loss [-0.10187217, -0.12800473, 0.026132561]\n",
      "Iter 1260, loss [-0.1296485, -0.1588956, 0.029247096]\n",
      "Iter 1261, loss [-0.31566742, -0.31680492, 0.0011374978]\n",
      "Iter 1262, loss [-0.13306977, -0.16089918, 0.027829414]\n",
      "Iter 1263, loss [-0.31742796, -0.31823555, 0.00080757577]\n",
      "Iter 1264, loss [-0.11748702, -0.14100383, 0.02351681]\n",
      "Iter 1265, loss [-0.13266015, -0.15659901, 0.02393887]\n",
      "Iter 1266, loss [-0.1214629, -0.14610666, 0.024643762]\n",
      "Iter 1267, loss [-0.12303716, -0.15546474, 0.03242758]\n",
      "Iter 1268, loss [-0.11983162, -0.14706402, 0.027232392]\n",
      "Iter 1269, loss [-0.12709449, -0.15145813, 0.024363637]\n",
      "Iter 1270, loss [-0.31983715, -0.32020316, 0.00036599318]\n",
      "Iter 1271, loss [-0.123887576, -0.14213496, 0.018247386]\n",
      "Iter 1272, loss [-0.13133088, -0.15441135, 0.023080476]\n",
      "Iter 1273, loss [-0.1297463, -0.15339668, 0.023650382]\n",
      "Iter 1274, loss [-0.13558319, -0.16088268, 0.025299486]\n",
      "Iter 1275, loss [-0.13312165, -0.16121535, 0.028093696]\n",
      "Iter 1276, loss [-0.122206576, -0.1492106, 0.027004024]\n",
      "Iter 1277, loss [-0.317713, -0.318564, 0.00085099787]\n",
      "Iter 1278, loss [-0.12484539, -0.15461081, 0.02976542]\n",
      "Iter 1279, loss [-0.1367686, -0.16172436, 0.024955768]\n",
      "Iter 1280, loss [-0.101578444, -0.1295199, 0.027941447]\n",
      "Iter 1281, loss [-0.14189586, -0.16791104, 0.02601517]\n",
      "Iter 1282, loss [-0.13313018, -0.16682594, 0.033695757]\n",
      "Iter 1283, loss [-0.13728598, -0.1628939, 0.025607927]\n",
      "Iter 1284, loss [-0.10323961, -0.13101982, 0.027780209]\n",
      "Iter 1285, loss [-0.14263804, -0.16753015, 0.024892112]\n",
      "Iter 1286, loss [-0.31802824, -0.31882694, 0.00079871406]\n",
      "Iter 1287, loss [-0.13367043, -0.15689367, 0.023223242]\n",
      "Iter 1288, loss [-0.14294113, -0.16660845, 0.02366732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1289, loss [-0.12603293, -0.15556784, 0.029534904]\n",
      "Iter 1290, loss [-0.13323054, -0.15847203, 0.025241492]\n",
      "Iter 1291, loss [-0.13776734, -0.16359714, 0.025829798]\n",
      "Iter 1292, loss [-0.104096144, -0.13160308, 0.027506933]\n",
      "Iter 1293, loss [-0.12422089, -0.16334718, 0.039126292]\n",
      "Iter 1294, loss [-0.3191069, -0.3196459, 0.0005390008]\n",
      "Iter 1295, loss [-0.13361077, -0.15927683, 0.025666064]\n",
      "Iter 1296, loss [-0.12361049, -0.15207718, 0.028466694]\n",
      "Iter 1297, loss [-0.13478222, -0.16251858, 0.02773635]\n",
      "Iter 1298, loss [-0.14524202, -0.17316917, 0.027927145]\n",
      "Iter 1299, loss [-0.10433386, -0.13222432, 0.027890462]\n",
      "Iter 1300, loss [-0.13498294, -0.16350733, 0.02852438]\n",
      "Iter 1301, loss [-0.1352529, -0.1641404, 0.028887516]\n",
      "Iter 1302, loss [-0.13426626, -0.16056037, 0.02629411]\n",
      "Iter 1303, loss [-0.14386773, -0.16863312, 0.02476538]\n",
      "Iter 1304, loss [-0.1050572, -0.13372964, 0.028672434]\n",
      "Iter 1305, loss [-0.123822376, -0.15249665, 0.028674271]\n",
      "Iter 1306, loss [-0.14593309, -0.1737976, 0.027864523]\n",
      "Iter 1307, loss [-0.10521678, -0.13367061, 0.028453832]\n",
      "Iter 1308, loss [-0.13566773, -0.16018628, 0.02451855]\n",
      "Iter 1309, loss [-0.12478211, -0.16517466, 0.040392555]\n",
      "Iter 1310, loss [-0.12770474, -0.15880732, 0.031102583]\n",
      "Iter 1311, loss [-0.13607636, -0.15976226, 0.023685908]\n",
      "Iter 1312, loss [-0.13702533, -0.1664715, 0.029446166]\n",
      "Iter 1313, loss [-0.13909817, -0.16494451, 0.025846345]\n",
      "Iter 1314, loss [-0.13484001, -0.1609032, 0.026063185]\n",
      "Iter 1315, loss [-0.12565465, -0.16415486, 0.0385002]\n",
      "Iter 1316, loss [-0.31891334, -0.3194436, 0.00053027575]\n",
      "Iter 1317, loss [-0.12437602, -0.15081498, 0.026438957]\n",
      "Iter 1318, loss [-0.13928121, -0.16411364, 0.024832428]\n",
      "Iter 1319, loss [-0.12463791, -0.15076338, 0.026125468]\n",
      "Iter 1320, loss [-0.104994, -0.1314014, 0.026407406]\n",
      "Iter 1321, loss [-0.13623546, -0.16079049, 0.024555027]\n",
      "Iter 1322, loss [-0.12861519, -0.15987273, 0.03125754]\n",
      "Iter 1323, loss [-0.14683135, -0.1755152, 0.028683852]\n",
      "Iter 1324, loss [-0.14464879, -0.16939965, 0.024750855]\n",
      "Iter 1325, loss [-0.13707419, -0.1641464, 0.027072206]\n",
      "Iter 1326, loss [-0.13738208, -0.16416557, 0.026783492]\n",
      "Iter 1327, loss [-0.13796745, -0.16729933, 0.029331874]\n",
      "Iter 1328, loss [-0.14037438, -0.16783746, 0.027463075]\n",
      "Iter 1329, loss [-0.13830057, -0.1672852, 0.028984644]\n",
      "Iter 1330, loss [-0.1450032, -0.16972584, 0.024722636]\n",
      "Iter 1331, loss [-0.13631403, -0.16459341, 0.028279377]\n",
      "Iter 1332, loss [-0.1283624, -0.15949705, 0.03113465]\n",
      "Iter 1333, loss [-0.106503755, -0.13602276, 0.029519007]\n",
      "Iter 1334, loss [-0.14509359, -0.17124179, 0.026148207]\n",
      "Iter 1335, loss [-0.14751783, -0.17623708, 0.028719246]\n",
      "Iter 1336, loss [-0.12548769, -0.15467885, 0.02919117]\n",
      "Iter 1337, loss [-0.12536636, -0.16636296, 0.04099659]\n",
      "Iter 1338, loss [-0.3183235, -0.31896248, 0.0006389779]\n",
      "Iter 1339, loss [-0.13904843, -0.16790979, 0.028861351]\n",
      "Iter 1340, loss [-0.1350896, -0.15962455, 0.024534946]\n",
      "Iter 1341, loss [-0.13498165, -0.15883128, 0.02384964]\n",
      "Iter 1342, loss [-0.105655834, -0.1321392, 0.026483372]\n",
      "Iter 1343, loss [-0.13575092, -0.16077702, 0.025026098]\n",
      "Iter 1344, loss [-0.14024244, -0.16471608, 0.024473641]\n",
      "Iter 1345, loss [-0.13589458, -0.16643035, 0.030535772]\n",
      "Iter 1346, loss [-0.31846166, -0.3191009, 0.0006392256]\n",
      "Iter 1347, loss [-0.10726247, -0.13877472, 0.031512253]\n",
      "Iter 1348, loss [-0.13784401, -0.16349663, 0.025652613]\n",
      "Iter 1349, loss [-0.14070524, -0.16876763, 0.028062388]\n",
      "Iter 1350, loss [-0.12899676, -0.16184613, 0.032849375]\n",
      "Iter 1351, loss [-0.1479754, -0.17637004, 0.028394645]\n",
      "Iter 1352, loss [-0.13820018, -0.16260089, 0.024400717]\n",
      "Iter 1353, loss [-0.12718004, -0.16597037, 0.038790327]\n",
      "Iter 1354, loss [-0.13909598, -0.166396, 0.027300034]\n",
      "Iter 1355, loss [-0.14088485, -0.1663227, 0.025437847]\n",
      "Iter 1356, loss [-0.12981951, -0.1598138, 0.0299943]\n",
      "Iter 1357, loss [-0.14839406, -0.17537753, 0.026983475]\n",
      "Iter 1358, loss [-0.12784214, -0.16577339, 0.03793125]\n",
      "Iter 1359, loss [-0.13556646, -0.16042292, 0.02485646]\n",
      "Iter 1360, loss [-0.12592247, -0.154573, 0.02865052]\n",
      "Iter 1361, loss [-0.14577782, -0.17063488, 0.024857052]\n",
      "Iter 1362, loss [-0.13645923, -0.16305014, 0.02659091]\n",
      "Iter 1363, loss [-0.13048676, -0.16280611, 0.032319352]\n",
      "Iter 1364, loss [-0.13972746, -0.16763635, 0.027908895]\n",
      "Iter 1365, loss [-0.1414635, -0.1691362, 0.027672693]\n",
      "Iter 1366, loss [-0.31853974, -0.31916973, 0.0006299765]\n",
      "Iter 1367, loss [-0.14866497, -0.17700537, 0.028340405]\n",
      "Iter 1368, loss [-0.14673959, -0.17257509, 0.025835505]\n",
      "Iter 1369, loss [-0.13718557, -0.16489391, 0.02770833]\n",
      "Iter 1370, loss [-0.10755308, -0.13708252, 0.029529437]\n",
      "Iter 1371, loss [-0.13838433, -0.1627829, 0.024398562]\n",
      "Iter 1372, loss [-0.14910369, -0.1773088, 0.028205117]\n",
      "Iter 1373, loss [-0.12620041, -0.15519787, 0.028997459]\n",
      "Iter 1374, loss [-0.12737815, -0.16749708, 0.04011893]\n",
      "Iter 1375, loss [-0.14660765, -0.17126447, 0.024656817]\n",
      "Iter 1376, loss [-0.14916474, -0.17544177, 0.026277041]\n",
      "Iter 1377, loss [-0.13040744, -0.16019851, 0.029791072]\n",
      "Iter 1378, loss [-0.12679133, -0.15406127, 0.02726995]\n",
      "Iter 1379, loss [-0.14190347, -0.16814448, 0.02624101]\n",
      "Iter 1380, loss [-0.14727439, -0.17321323, 0.025938835]\n",
      "Iter 1381, loss [-0.3193134, -0.31976795, 0.00045453967]\n",
      "Iter 1382, loss [-0.3193811, -0.3198345, 0.00045342167]\n",
      "Iter 1383, loss [-0.13859913, -0.16516776, 0.026568644]\n",
      "Iter 1384, loss [-0.31982818, -0.32022244, 0.00039424896]\n",
      "Iter 1385, loss [-0.13956518, -0.16589308, 0.026327899]\n",
      "Iter 1386, loss [-0.13095005, -0.16364783, 0.03269778]\n",
      "Iter 1387, loss [-0.14967974, -0.17737886, 0.027699124]\n",
      "Iter 1388, loss [-0.13138139, -0.16197194, 0.030590555]\n",
      "Iter 1389, loss [-0.13733864, -0.16643965, 0.02910101]\n",
      "Iter 1390, loss [-0.13422713, -0.15755582, 0.0233287]\n",
      "Iter 1391, loss [-0.13648072, -0.16200651, 0.02552579]\n",
      "Iter 1392, loss [-0.14211941, -0.16867737, 0.026557973]\n",
      "Iter 1393, loss [-0.13655788, -0.1665182, 0.029960325]\n",
      "Iter 1394, loss [-0.3174742, -0.31829697, 0.00082277553]\n",
      "Iter 1395, loss [-0.13081199, -0.1639264, 0.0331144]\n",
      "Iter 1396, loss [-0.14966094, -0.17843057, 0.028769631]\n",
      "Iter 1397, loss [-0.14746593, -0.17459697, 0.027131032]\n",
      "Iter 1398, loss [-0.14744228, -0.17387193, 0.026429659]\n",
      "Iter 1399, loss [-0.13993928, -0.16940324, 0.029463958]\n",
      "Iter 1400, loss [-0.13732128, -0.16399577, 0.0266745]\n",
      "Iter 1401, loss [-0.14773096, -0.17200087, 0.024269914]\n",
      "Iter 1402, loss [-0.14008078, -0.1680871, 0.028006308]\n",
      "Iter 1403, loss [-0.14039367, -0.16878338, 0.028389703]\n",
      "Iter 1404, loss [-0.14145975, -0.1673004, 0.02584066]\n",
      "Iter 1405, loss [-0.13751908, -0.16408075, 0.026561677]\n",
      "Iter 1406, loss [-0.1290318, -0.16822591, 0.0391941]\n",
      "Iter 1407, loss [-0.13178042, -0.16227098, 0.030490566]\n",
      "Iter 1408, loss [-0.13830489, -0.16162315, 0.023318265]\n",
      "Iter 1409, loss [-0.3203342, -0.32065395, 0.00031973977]\n",
      "Iter 1410, loss [-0.1376883, -0.16409855, 0.026410248]\n",
      "Iter 1411, loss [-0.14042948, -0.1698538, 0.029424325]\n",
      "Iter 1412, loss [-0.1427962, -0.17011897, 0.02732277]\n",
      "Iter 1413, loss [-0.1322662, -0.16382986, 0.031563666]\n",
      "Iter 1414, loss [-0.13815218, -0.16498488, 0.0268327]\n",
      "Iter 1415, loss [-0.12748584, -0.15549886, 0.028013026]\n",
      "Iter 1416, loss [-0.13935055, -0.16391197, 0.024561418]\n",
      "Iter 1417, loss [-0.12782231, -0.15608281, 0.028260492]\n",
      "Iter 1418, loss [-0.14284883, -0.17008625, 0.02723742]\n",
      "Iter 1419, loss [-0.14150666, -0.16883412, 0.027327463]\n",
      "Iter 1420, loss [-0.14173928, -0.1698387, 0.028099416]\n",
      "Iter 1421, loss [-0.13878794, -0.16706276, 0.02827482]\n",
      "Iter 1422, loss [-0.14990467, -0.17869583, 0.028791165]\n",
      "Iter 1423, loss [-0.13865666, -0.16840048, 0.029743817]\n",
      "Iter 1424, loss [-0.13819505, -0.16530326, 0.027108207]\n",
      "Iter 1425, loss [-0.32040727, -0.32070944, 0.00030216898]\n",
      "Iter 1426, loss [-0.13841403, -0.16491403, 0.026500002]\n",
      "Iter 1427, loss [-0.13848455, -0.16493021, 0.026445657]\n",
      "Iter 1428, loss [-0.14846244, -0.17331514, 0.02485269]\n",
      "Iter 1429, loss [-0.10818344, -0.13833368, 0.030150235]\n",
      "Iter 1430, loss [-0.1291144, -0.16964039, 0.040525988]\n",
      "Iter 1431, loss [-0.10852865, -0.13903828, 0.030509626]\n",
      "Iter 1432, loss [-0.14354, -0.16988313, 0.026343137]\n",
      "Iter 1433, loss [-0.14173253, -0.17217547, 0.030442934]\n",
      "Iter 1434, loss [-0.13873008, -0.16598481, 0.027254738]\n",
      "Iter 1435, loss [-0.32007805, -0.32042223, 0.00034418175]\n",
      "Iter 1436, loss [-0.14349174, -0.1691109, 0.025619147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1437, loss [-0.15061817, -0.1770863, 0.026468124]\n",
      "Iter 1438, loss [-0.32048604, -0.32076523, 0.0002791974]\n",
      "Iter 1439, loss [-0.13080955, -0.16729543, 0.03648588]\n",
      "Iter 1440, loss [-0.13801265, -0.1627938, 0.02478115]\n",
      "Iter 1441, loss [-0.109022796, -0.13807265, 0.029049857]\n",
      "Iter 1442, loss [-0.109313026, -0.13921367, 0.029900637]\n",
      "Iter 1443, loss [-0.32076836, -0.3210147, 0.00024635068]\n",
      "Iter 1444, loss [-0.13904071, -0.16859603, 0.029555324]\n",
      "Iter 1445, loss [-0.32084346, -0.3210832, 0.00023973276]\n",
      "Iter 1446, loss [-0.12983549, -0.17029148, 0.040456004]\n",
      "Iter 1447, loss [-0.12737143, -0.15683192, 0.029460488]\n",
      "Iter 1448, loss [-0.13943133, -0.16500074, 0.025569405]\n",
      "Iter 1449, loss [-0.13546485, -0.1572909, 0.021826064]\n",
      "Iter 1450, loss [-0.12797163, -0.15540753, 0.027435904]\n",
      "Iter 1451, loss [-0.13901462, -0.16622521, 0.027210599]\n",
      "Iter 1452, loss [-0.13883263, -0.16756257, 0.028729944]\n",
      "Iter 1453, loss [-0.13005821, -0.17100318, 0.040944964]\n",
      "Iter 1454, loss [-0.14377528, -0.17033288, 0.026557595]\n",
      "Iter 1455, loss [-0.13272932, -0.16405204, 0.03132271]\n",
      "Iter 1456, loss [-0.12872666, -0.15684246, 0.028115798]\n",
      "Iter 1457, loss [-0.15094344, -0.17865893, 0.027715495]\n",
      "Iter 1458, loss [-0.13956776, -0.16378392, 0.024216153]\n",
      "Iter 1459, loss [-0.14022298, -0.16478738, 0.024564398]\n",
      "Iter 1460, loss [-0.1408532, -0.17388235, 0.033029154]\n",
      "Iter 1461, loss [-0.148421, -0.17557918, 0.027158177]\n",
      "Iter 1462, loss [-0.1395202, -0.16873328, 0.029213091]\n",
      "Iter 1463, loss [-0.13950151, -0.16705438, 0.027552873]\n",
      "Iter 1464, loss [-0.31772557, -0.31846476, 0.0007391732]\n",
      "Iter 1465, loss [-0.13971075, -0.16530262, 0.025591861]\n",
      "Iter 1466, loss [-0.14154918, -0.16586488, 0.024315696]\n",
      "Iter 1467, loss [-0.138726, -0.16279426, 0.024068266]\n",
      "Iter 1468, loss [-0.32082143, -0.32106212, 0.00024069105]\n",
      "Iter 1469, loss [-0.13880062, -0.16334802, 0.02454739]\n",
      "Iter 1470, loss [-0.13214186, -0.1631721, 0.031030234]\n",
      "Iter 1471, loss [-0.13095741, -0.16849205, 0.037534643]\n",
      "Iter 1472, loss [-0.14419974, -0.17165025, 0.027450498]\n",
      "Iter 1473, loss [-0.14680769, -0.16951413, 0.022706449]\n",
      "Iter 1474, loss [-0.1486869, -0.17343058, 0.02474368]\n",
      "Iter 1475, loss [-0.14354713, -0.17286249, 0.029315349]\n",
      "Iter 1476, loss [-0.14086682, -0.17406286, 0.033196047]\n",
      "Iter 1477, loss [-0.14152452, -0.17381856, 0.032294042]\n",
      "Iter 1478, loss [-0.13954599, -0.16530235, 0.025756365]\n",
      "Iter 1479, loss [-0.13100922, -0.16915688, 0.038147666]\n",
      "Iter 1480, loss [-0.31810957, -0.31884563, 0.00073607266]\n",
      "Iter 1481, loss [-0.13142134, -0.16009606, 0.028674718]\n",
      "Iter 1482, loss [-0.14753506, -0.16919705, 0.021662]\n",
      "Iter 1483, loss [-0.32025757, -0.32062942, 0.00037184166]\n",
      "Iter 1484, loss [-0.13886073, -0.1639774, 0.025116667]\n",
      "Iter 1485, loss [-0.1370358, -0.15914553, 0.022109738]\n",
      "Iter 1486, loss [-0.147994, -0.17015432, 0.02216032]\n",
      "Iter 1487, loss [-0.1384874, -0.16255417, 0.024066782]\n",
      "Iter 1488, loss [-0.14317733, -0.1727876, 0.029610272]\n",
      "Iter 1489, loss [-0.14189631, -0.16846602, 0.02656971]\n",
      "Iter 1490, loss [-0.14426515, -0.17431861, 0.030053463]\n",
      "Iter 1491, loss [-0.14049843, -0.16772749, 0.027229063]\n",
      "Iter 1492, loss [-0.12858681, -0.15963234, 0.031045523]\n",
      "Iter 1493, loss [-0.3205463, -0.32083845, 0.00029215866]\n",
      "Iter 1494, loss [-0.13913822, -0.16863464, 0.029496415]\n",
      "Iter 1495, loss [-0.15022577, -0.1767233, 0.026497534]\n",
      "Iter 1496, loss [-0.3209257, -0.3211592, 0.00023351397]\n",
      "Iter 1497, loss [-0.14995952, -0.17457756, 0.024618048]\n",
      "Iter 1498, loss [-0.14174306, -0.17368683, 0.03194377]\n",
      "Iter 1499, loss [-0.13940671, -0.16472511, 0.025318407]\n",
      "Iter 1500, loss [-0.13130453, -0.16705678, 0.03575225]\n",
      "Iter 1501, loss [-0.10735005, -0.13443965, 0.027089596]\n",
      "Iter 1502, loss [-0.108122155, -0.13574569, 0.027623532]\n",
      "Iter 1503, loss [-0.1401784, -0.16749796, 0.027319564]\n"
     ]
    }
   ],
   "source": [
    "n_train_iters = 50000\n",
    "vol_gen = ds.gen_vols_batch(['labeled_train', 'unlabeled_train'], batch_size=1, randomize=True)\n",
    "print(ds.files_labeled_train + ds.files_unlabeled_train)\n",
    "#vol_gen = data_utils.gen_batch(X_unlabeled, X_unlabeled, batch_size=1, randomize=True)\n",
    "target_X, _ = next(vol_gen)\n",
    "zeros_flow = np.zeros(target_X.shape[:-1] + (3,))\n",
    "\n",
    "for bi in range(n_train_iters + 1):\n",
    "    \n",
    "    target_X, _ = next(vol_gen)\n",
    "    \n",
    "    # subject-to-atlas training\n",
    "    vm_losses = voxelmorph_model.train_on_batch([source_X, target_X], [target_X, zeros_flow])\n",
    "    #[source_X, target_X], [target_X, zeros_flow])\n",
    "    print('Iter {}, loss {}'.format(bi, vm_losses))\n",
    "    \n",
    "    if bi > 0 and bi % 2000 == 0:\n",
    "        voxelmorph_model.save('./experiments/voxelmorph/vm2_cc_AtoUMS_10-ul_xy_iter{}.h5'.format(start_iter + bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.display(PIL.Image.fromarray(source_X[0, :, :, 112, :]))\n",
    "IPython.display.display(PIL.Image.fromarray(target_X[0, :, :, 112, :]))\n",
    "preds = voxelmorph_model.predict([source_X, target_X])\n",
    "IPython.display.display(PIL.Image.fromarray(target_X[0, :, :, 112, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need a wrapper if we trained the old voxelmorph with ji indexing in the spatial transformer\n",
    "# # save a voxelmorph wrapper\n",
    "\n",
    "# from keras.layers import Input, Lambda\n",
    "# from keras.models import Model, load_model\n",
    "# import sys\n",
    "# sys.path.append('../voxelmorph/src')\n",
    "# from dense_3D_spatial_transformer import Dense3DSpatialTransformer\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# import keras.backend as K\n",
    "# gpu_ids = [2]\n",
    "# # set gpu id and tf settings\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(g) for g in gpu_ids])\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# K.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "\n",
    "# voxelmorph_model = load_model(\n",
    "#     #'/afs/csail.mit.edu/u/x/xamyzhao/voxelmorph/models/vm2_cc.h5',\n",
    "#     './experiments/voxelmorph/vm2_cc_AtoUMS_newdataset_iter100000.h5',#.format(start_iter),\n",
    "#     custom_objects={'Dense3DSpatialTransformer': Dense3DSpatialTransformer},\n",
    "#     compile=False,\n",
    "# )\n",
    "\n",
    "# vol_shape = (160, 224, 192, 1)\n",
    "# input_src = Input(vol_shape)\n",
    "# input_tgt = Input(vol_shape)\n",
    "\n",
    "# warped, flow = voxelmorph_model([input_src, input_tgt])\n",
    "# flow = Lambda(lambda x:tf.gather(x, [1, 0, 2], axis=-1))(flow)  # reverse rows, cols\n",
    "\n",
    "# wrapper_model = Model(inputs=[input_src, input_tgt], outputs=[flow, warped], name='vm2_cc_wrapper')\n",
    "# wrapper_model.summary()\n",
    "# wrapper_model.save('./experiments/voxelmorph/vm2_cc_AtoUMS_newdataset_iter100000_wrapper.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
